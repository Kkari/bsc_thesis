{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn.python.learn as learn\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import six.moves.cPickle as pickle\n",
    "import sys\n",
    "from pandas import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set= mnist.train.images.reshape(-1,28,28, 1)\n",
    "test_set = mnist.test.images.reshape(-1,28,28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(name, var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset, train_labels, test_dataset, test_labels, train_tensor,\n",
    "                accuracy, tf_batch_data, tf_batch_labels, log_dir='./logs',\n",
    "                num_steps=20000, batch_size=10, test_steps=1000, log_steps=100, predictor=None, last_test='np'):\n",
    "    with tf.Session() as session:\n",
    "        summaries = tf.merge_all_summaries()\n",
    "        train_writer = tf.train.SummaryWriter(log_dir + '/train', session.graph)\n",
    "        test_writer = tf.train.SummaryWriter(log_dir + '/test')\n",
    "\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        shuffle_train = np.random.permutation(train_dataset.shape[0])\n",
    "        train_dataset = train_dataset[shuffle_train]\n",
    "        train_labels = train_labels[shuffle_train]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = ((step * batch_size) % (train_labels.shape[0] - batch_size))\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {\n",
    "                tf_batch_data : batch_data, \n",
    "                tf_batch_labels : batch_labels,\n",
    "                keep_prob: 0.5\n",
    "            }\n",
    "    \n",
    "    \n",
    "            if step % test_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, acc, summary = session.run([train_tensor, accuracy, summaries], \n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata,\n",
    "                                             options=run_options)\n",
    "                print(\"Train accuracy at step %s: %.1f%%\" % (step, acc))\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                \n",
    "            elif step % log_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, summary = session.run([train_tensor, summaries], \n",
    "                                         feed_dict=feed_dict, \n",
    "                                         run_metadata=run_metadata,\n",
    "                                         options=run_options)\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                session.run(train_tensor, feed_dict=feed_dict, options=run_options)\n",
    "\n",
    "\n",
    "        feed_dict = {\n",
    "            tf_batch_data : test_dataset, \n",
    "            tf_batch_labels : test_labels,\n",
    "            keep_prob: 1\n",
    "        }\n",
    "        \n",
    "        if last_test == 'splitted':\n",
    "            predictions = np.empty([0,10])\n",
    "            for batch in np.array_split(test_dataset, test_dataset.shape[0] / 16):\n",
    "                tmp = session.run(predictor,\n",
    "                                          feed_dict={\n",
    "                                                tf_batch_data: batch,\n",
    "#                                                 batch_labels: np.array([]),\n",
    "                                                keep_prob: 1.0\n",
    "                })\n",
    "                predictions = np.vstack((predictions, tmp))\n",
    "            acc = accuracy_fn(predictions, test_labels)\n",
    "        elif accuracy is not None:   \n",
    "            acc = session.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Train accuracy: %.3f%%\" % acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc8/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32)\n",
      "Train accuracy at step 0: 0.1%\n",
      "Train accuracy at step 100: 0.7%\n",
      "Train accuracy at step 200: 0.8%\n",
      "Train accuracy at step 300: 0.8%\n",
      "Train accuracy at step 400: 0.9%\n",
      "Train accuracy at step 500: 0.9%\n",
      "Train accuracy at step 600: 0.9%\n",
      "Train accuracy at step 700: 1.0%\n",
      "Train accuracy at step 800: 1.0%\n",
      "Train accuracy at step 900: 1.0%\n"
     ]
    }
   ],
   "source": [
    "def convnet(inputs, keep_prob):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        \n",
    "        net = slim.conv2d(inputs, 32, [5, 5], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        \n",
    "        net = slim.conv2d(inputs, 64, [5, 5], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        \n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 1024, scope='fc6')\n",
    "        net = slim.dropout(net, 0.5, scope='dropout6')\n",
    "        \n",
    "#         net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "#         net = slim.dropout(net, 0.5, scope='dropout7')\n",
    "        \n",
    "        net = slim.fully_connected(net, 10, activation_fn=None, scope='fc8')\n",
    "        predictor = slim.softmax(net)\n",
    "    return net, predictor\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    last_layer, predictor = convnet(batch_data, keep_prob)\n",
    "    \n",
    "    print(last_layer)\n",
    "    print(batch_labels)\n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictor,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=train_set, \n",
    "                train_labels=mnist.train.labels, \n",
    "                test_dataset=test_set,\n",
    "                test_labels=mnist.test.labels, \n",
    "                train_tensor=train_tensor,\n",
    "                accuracy=accuracy,\n",
    "                last_test='splitted',\n",
    "                predictor=predictor,\n",
    "                tf_batch_data=batch_data, \n",
    "                tf_batch_labels=batch_labels,\n",
    "                batch_size=16, num_steps=20000, test_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[conv(5,32)-max(2,2)]*2 - flatten - 10, adams, dropout, 20k steps, l2=5e-3: 97,3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin-1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def from_flat_to_3d(image):\n",
    "#     print(image.shape)\n",
    "    return np.dstack((image[0:1024].reshape(32,32),\n",
    "                       image[1024:2048].reshape(32,32),\n",
    "                       image[2048:3072].reshape(32,32)))\n",
    "\n",
    "cifar_test = unpickle('cifar-10-batches-py/test_batch')\n",
    "cifar_test['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar_test['data']])\n",
    "\n",
    "cifar = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "for i in range(2, 6):\n",
    "    tmp = unpickle('cifar-10-batches-py/data_batch_' + str(i))\n",
    "    cifar['data'] = np.vstack((cifar['data'], tmp['data']))\n",
    "    cifar['labels'] = np.concatenate((cifar['labels'], tmp['labels']))\n",
    "\n",
    "cifar['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar['data']])\n",
    "\n",
    "cifar['data_bw'] = (cifar['data'][:,0:1024] + cifar['data'][:,1024:2048] + cifar['data'][:, 2048:3072]) / 3 \n",
    "cifar_test['data_bw'] = (cifar_test['data'][:,0:1024] + cifar_test['data'][:,1024:2048] + cifar_test['data'][:, 2048:3072]) / 3 \n",
    "\n",
    "enc = OneHotEncoder()\n",
    "cifar['labels_oh'] = enc.fit_transform(cifar['labels'].reshape(-1, 1))\n",
    "cifar['labels_oh'] = cifar['labels_oh'].toarray()\n",
    "\n",
    "cifar_test['labels'] = np.array(cifar_test['labels'])\n",
    "cifar_test['labels_oh'] = enc.fit_transform(cifar_test['labels'].reshape(-1, 1))\n",
    "cifar_test['labels_oh'] = cifar_test['labels_oh'].toarray()\n",
    "\n",
    "pca = PCA(whiten=True)\n",
    "cifar['data_bw_whitened'] = pca.fit_transform(cifar['data_bw'])\n",
    "cifar_test['data_bw_whitened'] = pca.fit_transform(cifar_test['data_bw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar['data_3d'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc8/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"batch_labels:0\", shape=(?, 10), dtype=float32)\n",
      "Train accuracy at step 0: 0.0%\n",
      "Train accuracy at step 100: 0.1%\n",
      "Train accuracy at step 200: 0.2%\n",
      "Train accuracy at step 300: 0.3%\n",
      "Train accuracy at step 400: 0.2%\n",
      "Train accuracy at step 500: 0.4%\n",
      "Train accuracy at step 600: 0.1%\n",
      "Train accuracy at step 700: 0.4%\n",
      "Train accuracy at step 800: 0.3%\n",
      "Train accuracy at step 900: 0.1%\n",
      "Train accuracy at step 1000: 0.5%\n",
      "Train accuracy at step 1100: 0.5%\n",
      "Train accuracy at step 1200: 0.5%\n",
      "Train accuracy at step 1300: 0.2%\n",
      "Train accuracy at step 1400: 0.6%\n",
      "Train accuracy at step 1500: 0.6%\n",
      "Train accuracy at step 1600: 0.3%\n",
      "Train accuracy at step 1700: 0.3%\n",
      "Train accuracy at step 1800: 0.4%\n",
      "Train accuracy at step 1900: 0.5%\n",
      "Train accuracy at step 2000: 0.3%\n",
      "Train accuracy at step 2100: 0.2%\n",
      "Train accuracy at step 2200: 0.8%\n",
      "Train accuracy at step 2300: 0.4%\n",
      "Train accuracy at step 2400: 0.7%\n",
      "Train accuracy at step 2500: 0.4%\n",
      "Train accuracy at step 2600: 0.3%\n",
      "Train accuracy at step 2700: 0.6%\n",
      "Train accuracy at step 2800: 0.6%\n",
      "Train accuracy at step 2900: 0.6%\n",
      "Train accuracy at step 3000: 0.4%\n",
      "Train accuracy at step 3100: 0.4%\n",
      "Train accuracy at step 3200: 0.3%\n",
      "Train accuracy at step 3300: 0.6%\n",
      "Train accuracy at step 3400: 0.4%\n",
      "Train accuracy at step 3500: 0.4%\n",
      "Train accuracy at step 3600: 0.6%\n",
      "Train accuracy at step 3700: 0.6%\n",
      "Train accuracy at step 3800: 0.6%\n",
      "Train accuracy at step 3900: 0.4%\n",
      "Train accuracy at step 4000: 0.5%\n",
      "Train accuracy at step 4100: 0.3%\n",
      "Train accuracy at step 4200: 0.6%\n",
      "Train accuracy at step 4300: 0.4%\n",
      "Train accuracy at step 4400: 0.4%\n",
      "Train accuracy at step 4500: 0.6%\n",
      "Train accuracy at step 4600: 0.1%\n",
      "Train accuracy at step 4700: 0.5%\n",
      "Train accuracy at step 4800: 0.6%\n",
      "Train accuracy at step 4900: 0.6%\n",
      "Train accuracy at step 5000: 0.4%\n",
      "Train accuracy at step 5100: 0.5%\n",
      "Train accuracy at step 5200: 0.5%\n",
      "Train accuracy at step 5300: 0.6%\n",
      "Train accuracy at step 5400: 0.3%\n",
      "Train accuracy at step 5500: 0.6%\n",
      "Train accuracy at step 5600: 0.3%\n",
      "Train accuracy at step 5700: 0.5%\n",
      "Train accuracy at step 5800: 0.6%\n",
      "Train accuracy at step 5900: 0.4%\n",
      "Train accuracy at step 6000: 0.5%\n",
      "Train accuracy at step 6100: 0.4%\n",
      "Train accuracy at step 6200: 0.4%\n",
      "Train accuracy at step 6300: 0.4%\n",
      "Train accuracy at step 6400: 0.4%\n",
      "Train accuracy at step 6500: 0.6%\n",
      "Train accuracy at step 6600: 0.4%\n",
      "Train accuracy at step 6700: 0.6%\n",
      "Train accuracy at step 6800: 0.6%\n",
      "Train accuracy at step 6900: 0.8%\n",
      "Train accuracy at step 7000: 0.4%\n",
      "Train accuracy at step 7100: 0.6%\n",
      "Train accuracy at step 7200: 0.4%\n",
      "Train accuracy at step 7300: 0.4%\n",
      "Train accuracy at step 7400: 0.5%\n",
      "Train accuracy at step 7500: 0.4%\n",
      "Train accuracy at step 7600: 0.5%\n",
      "Train accuracy at step 7700: 0.5%\n",
      "Train accuracy at step 7800: 0.4%\n",
      "Train accuracy at step 7900: 0.4%\n",
      "Train accuracy at step 8000: 0.6%\n",
      "Train accuracy at step 8100: 0.5%\n",
      "Train accuracy at step 8200: 0.4%\n",
      "Train accuracy at step 8300: 0.6%\n",
      "Train accuracy at step 8400: 0.5%\n",
      "Train accuracy at step 8500: 0.6%\n",
      "Train accuracy at step 8600: 0.6%\n",
      "Train accuracy at step 8700: 0.4%\n",
      "Train accuracy at step 8800: 0.5%\n",
      "Train accuracy at step 8900: 0.5%\n",
      "Train accuracy at step 9000: 0.6%\n",
      "Train accuracy at step 9100: 0.6%\n",
      "Train accuracy at step 9200: 0.5%\n",
      "Train accuracy at step 9300: 0.6%\n",
      "Train accuracy at step 9400: 0.6%\n",
      "Train accuracy at step 9500: 0.4%\n",
      "Train accuracy at step 9600: 0.3%\n",
      "Train accuracy at step 9700: 0.5%\n",
      "Train accuracy at step 9800: 0.6%\n",
      "Train accuracy at step 9900: 0.5%\n",
      "Train accuracy at step 10000: 0.6%\n",
      "Train accuracy at step 10100: 0.7%\n",
      "Train accuracy at step 10200: 0.5%\n",
      "Train accuracy at step 10300: 0.6%\n",
      "Train accuracy at step 10400: 0.6%\n",
      "Train accuracy at step 10500: 0.7%\n",
      "Train accuracy at step 10600: 0.8%\n",
      "Train accuracy at step 10700: 0.7%\n",
      "Train accuracy at step 10800: 0.6%\n",
      "Train accuracy at step 10900: 0.6%\n",
      "Train accuracy at step 11000: 0.2%\n",
      "Train accuracy at step 11100: 0.5%\n",
      "Train accuracy at step 11200: 0.6%\n",
      "Train accuracy at step 11300: 0.6%\n",
      "Train accuracy at step 11400: 0.4%\n",
      "Train accuracy at step 11500: 0.5%\n",
      "Train accuracy at step 11600: 0.7%\n",
      "Train accuracy at step 11700: 0.3%\n",
      "Train accuracy at step 11800: 0.4%\n",
      "Train accuracy at step 11900: 0.6%\n",
      "Train accuracy at step 12000: 0.4%\n",
      "Train accuracy at step 12100: 0.8%\n",
      "Train accuracy at step 12200: 0.6%\n",
      "Train accuracy at step 12300: 0.3%\n",
      "Train accuracy at step 12400: 0.5%\n",
      "Train accuracy at step 12500: 0.4%\n",
      "Train accuracy at step 12600: 0.7%\n",
      "Train accuracy at step 12700: 0.7%\n",
      "Train accuracy at step 12800: 0.7%\n",
      "Train accuracy at step 12900: 0.3%\n",
      "Train accuracy at step 13000: 0.6%\n",
      "Train accuracy at step 13100: 0.4%\n",
      "Train accuracy at step 13200: 0.3%\n",
      "Train accuracy at step 13300: 0.6%\n",
      "Train accuracy at step 13400: 0.6%\n",
      "Train accuracy at step 13500: 0.4%\n",
      "Train accuracy at step 13600: 0.6%\n",
      "Train accuracy at step 13700: 0.3%\n",
      "Train accuracy at step 13800: 0.4%\n",
      "Train accuracy at step 13900: 0.6%\n",
      "Train accuracy at step 14000: 0.6%\n",
      "Train accuracy at step 14100: 0.6%\n",
      "Train accuracy at step 14200: 0.4%\n",
      "Train accuracy at step 14300: 0.4%\n",
      "Train accuracy at step 14400: 0.7%\n",
      "Train accuracy at step 14500: 0.6%\n",
      "Train accuracy at step 14600: 0.6%\n",
      "Train accuracy at step 14700: 0.6%\n",
      "Train accuracy at step 14800: 0.5%\n",
      "Train accuracy at step 14900: 0.6%\n",
      "Train accuracy at step 15000: 0.4%\n",
      "Train accuracy at step 15100: 0.6%\n",
      "Train accuracy at step 15200: 0.7%\n",
      "Train accuracy at step 15300: 0.5%\n",
      "Train accuracy at step 15400: 0.7%\n",
      "Train accuracy at step 15500: 0.4%\n",
      "Train accuracy at step 15600: 0.5%\n",
      "Train accuracy at step 15700: 0.8%\n",
      "Train accuracy at step 15800: 0.4%\n",
      "Train accuracy at step 15900: 0.5%\n",
      "Train accuracy at step 16000: 0.8%\n",
      "Train accuracy at step 16100: 0.5%\n",
      "Train accuracy at step 16200: 0.4%\n",
      "Train accuracy at step 16300: 0.4%\n",
      "Train accuracy at step 16400: 0.6%\n",
      "Train accuracy at step 16500: 0.8%\n",
      "Train accuracy at step 16600: 0.6%\n",
      "Train accuracy at step 16700: 0.6%\n",
      "Train accuracy at step 16800: 0.8%\n",
      "Train accuracy at step 16900: 0.6%\n",
      "Train accuracy at step 17000: 0.4%\n",
      "Train accuracy at step 17100: 0.4%\n",
      "Train accuracy at step 17200: 0.9%\n",
      "Train accuracy at step 17300: 0.6%\n",
      "Train accuracy at step 17400: 0.4%\n",
      "Train accuracy at step 17500: 0.8%\n",
      "Train accuracy at step 17600: 0.6%\n",
      "Train accuracy at step 17700: 0.4%\n",
      "Train accuracy at step 17800: 0.4%\n",
      "Train accuracy at step 17900: 0.7%\n",
      "Train accuracy at step 18000: 0.6%\n",
      "Train accuracy at step 18100: 0.6%\n",
      "Train accuracy at step 18200: 0.6%\n",
      "Train accuracy at step 18300: 0.7%\n",
      "Train accuracy at step 18400: 0.4%\n",
      "Train accuracy at step 18500: 0.5%\n",
      "Train accuracy at step 18600: 0.4%\n",
      "Train accuracy at step 18700: 0.6%\n",
      "Train accuracy at step 18800: 0.6%\n",
      "Train accuracy at step 18900: 0.7%\n",
      "Train accuracy at step 19000: 0.5%\n",
      "Train accuracy at step 19100: 0.6%\n",
      "Train accuracy at step 19200: 0.6%\n",
      "Train accuracy at step 19300: 0.7%\n",
      "Train accuracy at step 19400: 0.8%\n",
      "Train accuracy at step 19500: 0.5%\n",
      "Train accuracy at step 19600: 0.4%\n",
      "Train accuracy at step 19700: 0.7%\n",
      "Train accuracy at step 19800: 0.8%\n",
      "Train accuracy at step 19900: 0.7%\n",
      "Train accuracy: 52.440%\n"
     ]
    }
   ],
   "source": [
    "def convnet(inputs, keep_prob):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        \n",
    "        net = slim.conv2d(inputs, 32, [5, 5], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        \n",
    "        net = slim.conv2d(inputs, 64, [5, 5], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        \n",
    "        net = slim.flatten(net)\n",
    "#         net = slim.fully_connected(net, 4096, scope='fc6')\n",
    "        net = slim.dropout(net, 0.5, scope='dropout6')\n",
    "        \n",
    "#         net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "#         net = slim.dropout(net, 0.5, scope='dropout7')\n",
    "        \n",
    "        net = slim.fully_connected(net, 10, activation_fn=None, scope='fc8')\n",
    "        predictor = slim.softmax(net)\n",
    "    return net, predictor\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels), name='batch_data')\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='batch_labels')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    last_layer, predictor = convnet(batch_data, keep_prob)\n",
    "    \n",
    "    print(last_layer)\n",
    "    print(batch_labels)\n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictor,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=cifar['data_3d'], \n",
    "                train_labels=cifar['labels_oh'], \n",
    "                test_dataset=cifar_test['data_3d'],\n",
    "                test_labels=cifar_test['labels_oh'], \n",
    "                train_tensor=train_tensor,\n",
    "                accuracy=accuracy,\n",
    "                last_test='splitted',\n",
    "                predictor=predictor,\n",
    "                tf_batch_data=batch_data, \n",
    "                tf_batch_labels=batch_labels,\n",
    "                log_dir='cnn_cifar_1',\n",
    "                batch_size=16, num_steps=20000, test_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-43-fe8ef3147aaa>\", line 2, in <module>\n",
      "    predictions = session.run(predictions,\n",
      "NameError: name 'session' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 709, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for batch in np.array_split(cifar_test['data_3d'], cifar_test['data_3d'].shape[0] / 16):\n",
    "    predictions = session.run(predictions,\n",
    "                              feed_dict={\n",
    "                                    batch_data: batch,\n",
    "                                    batch_labels: np.array([]),\n",
    "                                    keep_prob: 1.0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

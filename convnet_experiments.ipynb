{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn.python.learn as learn\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import six.moves.cPickle as pickle\n",
    "import sys\n",
    "from pandas import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_fn(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set= mnist.train.images.reshape(-1,28,28, 1)\n",
    "test_set = mnist.test.images.reshape(-1,28,28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(name, var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset, train_labels, test_dataset, test_labels, train_tensor,\n",
    "                accuracy, tf_batch_data, tf_batch_labels, log_dir='./logs',\n",
    "                num_steps=20000, batch_size=10, test_steps=1000, log_steps=100, predictor=None, last_test='np'):\n",
    "    with tf.Session() as session:\n",
    "        summaries = tf.merge_all_summaries()\n",
    "\n",
    "        if tf.gfile.Exists(log_dir):\n",
    "            tf.gfile.DeleteRecursively(log_dir)\n",
    "            \n",
    "        train_writer = tf.train.SummaryWriter(log_dir + '/train', session.graph)\n",
    "        test_writer = tf.train.SummaryWriter(log_dir + '/test')\n",
    "\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        shuffle_train = np.random.permutation(train_dataset.shape[0])\n",
    "        train_dataset = train_dataset[shuffle_train]\n",
    "        train_labels = train_labels[shuffle_train]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = ((step * batch_size) % (train_labels.shape[0] - batch_size))\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {\n",
    "                tf_batch_data : batch_data, \n",
    "                tf_batch_labels : batch_labels,\n",
    "                keep_prob: 0.5\n",
    "            }\n",
    "    \n",
    "    \n",
    "            if step % test_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, acc, summary = session.run([train_tensor, accuracy, summaries], \n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata,\n",
    "                                             options=run_options)\n",
    "                print(\"Train accuracy at step %s: %.1f%%\" % (step, acc))\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                \n",
    "            elif step % log_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, summary = session.run([train_tensor, summaries], \n",
    "                                         feed_dict=feed_dict, \n",
    "                                         run_metadata=run_metadata,\n",
    "                                         options=run_options)\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                session.run(train_tensor, feed_dict=feed_dict, options=run_options)\n",
    "\n",
    "\n",
    "        feed_dict = {\n",
    "            tf_batch_data : test_dataset, \n",
    "            tf_batch_labels : test_labels,\n",
    "            keep_prob: 1\n",
    "        }\n",
    "        \n",
    "        if last_test == 'splitted':\n",
    "            predictions = np.empty([0,10])\n",
    "            for batch in np.array_split(test_dataset, test_dataset.shape[0] / 16):\n",
    "                tmp = session.run(predictor,\n",
    "                                          feed_dict={\n",
    "                                                tf_batch_data: batch,\n",
    "#                                                 batch_labels: np.array([]),\n",
    "                                                keep_prob: 1.0\n",
    "                })\n",
    "                predictions = np.vstack((predictions, tmp))\n",
    "            acc = accuracy_fn(predictions, test_labels)\n",
    "        elif accuracy is not None:   \n",
    "            acc = session.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Test accuracy: %.3f%%\" % acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc8/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32)\n",
      "Train accuracy at step 0: 0.0%\n",
      "Train accuracy at step 100: 0.9%\n",
      "Train accuracy at step 200: 1.0%\n",
      "Train accuracy at step 300: 0.8%\n"
     ]
    }
   ],
   "source": [
    "def convnet(inputs, keep_prob):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        \n",
    "        net = slim.conv2d(inputs, 32, [5, 5], scope='conv1')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "        \n",
    "        net = slim.conv2d(net, 64, [5, 5], scope='conv2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "        \n",
    "#         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "#         net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "        \n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 1024, scope='fc6')\n",
    "        net = slim.dropout(net, keep_prob, scope='dropout6')\n",
    "        \n",
    "#         net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "#         net = slim.dropout(net, 0.5, scope='dropout7')\n",
    "        \n",
    "        net = slim.fully_connected(net, 10, activation_fn=None, scope='fc8')\n",
    "        predictor = slim.softmax(net)\n",
    "    return net, predictor\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    last_layer, predictor = convnet(batch_data, keep_prob)\n",
    "    \n",
    "    print(last_layer)\n",
    "    print(batch_labels)\n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    tf.scalar_summary('accuracy', accuracy)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictor,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "    train_model(train_dataset=train_set, \n",
    "                train_labels=mnist.train.labels, \n",
    "                test_dataset=test_set,\n",
    "                test_labels=mnist.test.labels, \n",
    "                train_tensor=train_tensor,\n",
    "                accuracy=accuracy,\n",
    "                last_test='splitted',\n",
    "                predictor=predictor,\n",
    "                tf_batch_data=batch_data,\n",
    "                log_dir='mnist_conv_max_conv_max_flatten_fc_d_sm_autoADAM',\n",
    "                tf_batch_labels=batch_labels,\n",
    "                batch_size=16, num_steps=20000, test_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [conv(5,32)-max(2,2)]*1 - flatten - 10, adams, dropout, 20k steps, l2=5e-3: 2.7%\n",
    "* [conv(5,32)-max(2,2)]*1 - flatten - fully_1024 - 10, adams, dropout, 20k steps, l2=5e-3: 1.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin-1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def from_flat_to_3d(image):\n",
    "#     print(image.shape)\n",
    "    return np.dstack((image[0:1024].reshape(32,32),\n",
    "                       image[1024:2048].reshape(32,32),\n",
    "                       image[2048:3072].reshape(32,32)))\n",
    "\n",
    "cifar_test = unpickle('cifar-10-batches-py/test_batch')\n",
    "cifar_test['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar_test['data']])\n",
    "\n",
    "cifar = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "for i in range(2, 6):\n",
    "    tmp = unpickle('cifar-10-batches-py/data_batch_' + str(i))\n",
    "    cifar['data'] = np.vstack((cifar['data'], tmp['data']))\n",
    "    cifar['labels'] = np.concatenate((cifar['labels'], tmp['labels']))\n",
    "\n",
    "cifar['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar['data']])\n",
    "\n",
    "# cifar['data_bw'] = (cifar['data'][:,0:1024] + cifar['data'][:,1024:2048] + cifar['data'][:, 2048:3072]) / 3 \n",
    "# cifar_test['data_bw'] = (cifar_test['data'][:,0:1024] + cifar_test['data'][:,1024:2048] + cifar_test['data'][:, 2048:3072]) / 3 \n",
    "\n",
    "enc = OneHotEncoder()\n",
    "cifar['labels_oh'] = enc.fit_transform(cifar['labels'].reshape(-1, 1))\n",
    "cifar['labels_oh'] = cifar['labels_oh'].toarray()\n",
    "\n",
    "cifar_test['labels'] = np.array(cifar_test['labels'])\n",
    "cifar_test['labels_oh'] = enc.fit_transform(cifar_test['labels'].reshape(-1, 1))\n",
    "cifar_test['labels_oh'] = cifar_test['labels_oh'].toarray()\n",
    "\n",
    "# pca = PCA(whiten=True)\n",
    "# cifar['data_bw_whitened'] = pca.fit_transform(cifar['data_bw'])\n",
    "# cifar_test['data_bw_whitened'] = pca.fit_transform(cifar_test['data_bw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar['data_3d'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc8/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"batch_labels:0\", shape=(?, 10), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device to node 'fc8/biases/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\nColocation Debug Info:\nColocation group had the following types and devices: \nApplyAdam: CPU \nConst: CPU \nIdentity: CPU \nAssign: CPU \nVariable: CPU \n\t [[Node: fc8/biases/Adam_1 = Variable[_class=[\"loc:@fc8/biases\"], container=\"\", dtype=DT_FLOAT, shape=[10], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\nCaused by op 'fc8/biases/Adam_1', defined at:\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-0f140f523f80>\", line 61, in <module>\n    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 457, in create_train_op\n    grad_updates = optimizer.apply_gradients(grads, global_step=global_step)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/adam.py\", line 119, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 494, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 108, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 86, in create_slot\n    return _create_slot_var(primary, val, scope)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 50, in _create_slot_var\n    slot = variables.Variable(val, name=scope, trainable=False)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 296, in _init_from_args\n    name=name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 140, in variable_op\n    container=container, shared_name=shared_name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 396, in _variable\n    name=name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'fc8/biases/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\nColocation Debug Info:\nColocation group had the following types and devices: \nApplyAdam: CPU \nConst: CPU \nIdentity: CPU \nAssign: CPU \nVariable: CPU \n\t [[Node: fc8/biases/Adam_1 = Variable[_class=[\"loc:@fc8/biases\"], container=\"\", dtype=DT_FLOAT, shape=[10], shared_name=\"\", _device=\"/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0f140f523f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtf_batch_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cifar_conv_max_lrn_conv_lrn_max_flatten_fc_d_fc_d_sm_autoADAM_gpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 batch_size=32, num_steps=30000, test_steps=1000)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2de30c81c763>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataset, train_labels, test_dataset, test_labels, train_tensor, accuracy, tf_batch_data, tf_batch_labels, log_dir, num_steps, batch_size, test_steps, log_steps, predictor, last_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtest_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mshuffle_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device to node 'fc8/biases/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\nColocation Debug Info:\nColocation group had the following types and devices: \nApplyAdam: CPU \nConst: CPU \nIdentity: CPU \nAssign: CPU \nVariable: CPU \n\t [[Node: fc8/biases/Adam_1 = Variable[_class=[\"loc:@fc8/biases\"], container=\"\", dtype=DT_FLOAT, shape=[10], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\nCaused by op 'fc8/biases/Adam_1', defined at:\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-0f140f523f80>\", line 61, in <module>\n    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 457, in create_train_op\n    grad_updates = optimizer.apply_gradients(grads, global_step=global_step)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/adam.py\", line 119, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 494, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 108, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 86, in create_slot\n    return _create_slot_var(primary, val, scope)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/slot_creator.py\", line 50, in _create_slot_var\n    slot = variables.Variable(val, name=scope, trainable=False)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 296, in _init_from_args\n    name=name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\", line 140, in variable_op\n    container=container, shared_name=shared_name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 396, in _variable\n    name=name)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "def convnet(inputs, keep_prob, is_training):\n",
    "    with tf.device('gpu:0'):\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                          activation_fn=tf.nn.relu,\n",
    "                          weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                          weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "\n",
    "            net = slim.conv2d(inputs, 32, [5, 5], scope='conv1')\n",
    "            variable_summaries('conv1', net)\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "            net = slim.conv2d(net, 64, [5, 5], scope='conv2')\n",
    "            variable_summaries('conv2', net)\n",
    "            net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')      \n",
    "    #         net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "    #         net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "\n",
    "    #         net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "    #         net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "\n",
    "    #         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "    #         net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "\n",
    "    #         net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "    #          net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "\n",
    "            net = slim.flatten(net)\n",
    "            net = slim.fully_connected(net, 1024, scope='fc6')\n",
    "            variable_summaries('fc1', net)\n",
    "            net = slim.dropout(net, keep_prob, scope='dropout6')\n",
    "\n",
    "            net = slim.fully_connected(net, 1024, scope='fc7')\n",
    "            variable_summaries('fc2', net)\n",
    "            net = slim.dropout(net, keep_prob, scope='dropout7')\n",
    "\n",
    "            net = slim.fully_connected(net, 10, activation_fn=None, scope='fc8')\n",
    "            predictor = slim.softmax(net)\n",
    "        return net, predictor\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels), name='batch_data')\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels), name='batch_labels')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    last_layer, predictor = convnet(batch_data, keep_prob, is_training)\n",
    "    \n",
    "    print(last_layer)\n",
    "    print(batch_labels)\n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictor,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "    train_model(train_dataset=cifar['data_3d'], \n",
    "                train_labels=cifar['labels_oh'], \n",
    "                test_dataset=cifar_test['data_3d'],\n",
    "                test_labels=cifar_test['labels_oh'], \n",
    "                train_tensor=train_tensor,\n",
    "                accuracy=accuracy,\n",
    "                last_test='splitted',\n",
    "                predictor=predictor,\n",
    "                tf_batch_data=batch_data, \n",
    "                tf_batch_labels=batch_labels,\n",
    "                log_dir='cifar_conv_max_lrn_conv_lrn_max_flatten_fc_d_fc_d_sm_autoADAM_gpu',\n",
    "                batch_size=32, num_steps=30000, test_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cifar_conv_lrn_max_flatten_fc_d_fc_d_sm_autoADAMTest accuracy: 66.770%\n",
    "* cifar_conv_max_flatten_fc_d_fc_d_sm_autoADAMTest accuracy: 66.480%\n",
    "* cifar_conv_max_conv_max_flatten_fc_d_fc_d_sm_autoADAM accuracy:  73.090%\n",
    "* cifar_conv_max_lrn_conv_lrn_max_flatten_fc_d_fc_d_sm_autoADAM: 74.040%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-65.77\n",
      "-65.48\n",
      "-72.09\n"
     ]
    }
   ],
   "source": [
    "print(1 - 66.77)\n",
    "print(1 - 66.48)\n",
    "print(1 - 73.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3d27bbcb6453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_3d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_3d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     tmp = session.run(predictor,\n\u001b[0m\u001b[1;32m      4\u001b[0m                               feed_dict={\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mtf_batch_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = np.empty([0,10])\n",
    "for batch in np.array_split(cifar_test['data_3d'], cifar_test['data_3d'].shape[0] / 16):\n",
    "    tmp = session.run(predictor,\n",
    "                              feed_dict={\n",
    "                                    tf_batch_data: batch,\n",
    "#                                   batch_labels: np.array([]),\n",
    "                                    keep_prob: 1.0\n",
    "    })\n",
    "    predictions = np.vstack((predictions, tmp))\n",
    "acc = accuracy_fn(predictions, cifar_test['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [conv(5,32)-max(2,2)]*2 - flatten - 10, adams, dropout, 20k steps, l2=5e-3: 52.4%\n",
    "* [conv(5,32)-max(2,2)]*2 - flatten - fc(1024) - 10, adams, dropout, 20k steps, l2=5e-3: 66.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-43-fe8ef3147aaa>\", line 2, in <module>\n",
      "    predictions = session.run(predictions,\n",
      "NameError: name 'session' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/kkari/DevTools/anaconda3/lib/python3.5/inspect.py\", line 709, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for batch in np.array_split(cifar_test['data_3d'], cifar_test['data_3d'].shape[0] / 16):\n",
    "    predictions = session.run(predictions,\n",
    "                              feed_dict={\n",
    "                                    batch_data: batch,\n",
    "                                    batch_labels: np.array([]),\n",
    "                                    keep_prob: 1.0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

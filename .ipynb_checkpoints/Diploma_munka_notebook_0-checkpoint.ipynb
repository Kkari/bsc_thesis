{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# Reformat the dataset for the convolutional networks\n",
    "def reformat(dataset):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# The mnist images have a dimension of 28*28. \n",
    "image_size = 28\n",
    "# There are 10 labels.\n",
    "num_labels = 10\n",
    "train_dataset = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "\n",
    "perm = np.random.permutation(mnist.test.images.shape[0])\n",
    "\n",
    "split_point = int(mnist.test.images.shape[0] * 0.1)\n",
    "valid_dataset, test_dataset = mnist.test.images[:split_point], mnist.test.images[split_point:]\n",
    "valid_labels, test_labels = mnist.test.labels[:split_point], mnist.test.labels[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple logistic regression a' la sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a baseline with a simple logistic regression, ommiting reguralization. Just to see where we are starting from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels_not_hot = np.nonzero(mnist.train.labels)[1]\n",
    "test_labels_not_hot = np.nonzero(mnist.test.labels[split_point:])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9211111111111111"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_dataset, train_labels_not_hot)\n",
    "lr.score(test_dataset, test_labels_not_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('input'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    with tf.name_scope('hidden'):\n",
    "        weights_hidden = tf.Variable(tf.truncated_normal([image_size * image_size, 1024], stddev=0.1), \n",
    "                                   name='weights')\n",
    "        biases_hidden = tf.Variable(tf.constant(0.1, shape=[1024]), name='biases')\n",
    "        relu_output = tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden) + biases_hidden)\n",
    "\n",
    "    with tf.name_scope('output'):\n",
    "        weights_output = tf.Variable(tf.truncated_normal([1024, num_labels], stddev=0.1), name='weights')\n",
    "        biases_output = tf.Variable(tf.constant(0.1, shape=[num_labels]), name='biases')\n",
    "        logits = tf.matmul(relu_output, weights_output) + biases_output\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(\n",
    "                                                tf.matmul(tf_valid_dataset, weights_hidden) + \n",
    "                                                biases_hidden),\n",
    "                                               weights_output) + \n",
    "                                     biases_output)\n",
    "\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(\n",
    "                                                tf.matmul(tf_test_dataset, weights_hidden) + \n",
    "                                                biases_hidden),\n",
    "                                               weights_output) + \n",
    "                                    biases_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.947026\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 13.7%\n",
      "Minibatch loss at step 500: 0.117899\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 96.0%\n",
      "Minibatch loss at step 1000: 0.018200\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.0%\n",
      "Minibatch loss at step 1500: 0.094146\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 97.3%\n",
      "Minibatch loss at step 2000: 0.052630\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.6%\n",
      "Minibatch loss at step 2500: 0.018096\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.9%\n",
      "Minibatch loss at step 3000: 0.004794\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.0%\n",
      "Test accuracy: 98.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "       \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],\n",
    "                                        feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "       \n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.merge_all_summaries()\n",
    "    train_writer = tf.train.SummaryWriter('./train',\n",
    "                                        session.graph)\n",
    "    test_writer = tf.train.SummaryWriter('./test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same network, but with dropout and l2 reguralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    with tf.name_scope('input'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, image_size * image_size))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope('hidden'):\n",
    "        weights_hidden = tf.Variable(tf.truncated_normal([image_size * image_size, 1024], stddev=0.1),\n",
    "                                     name='weights')\n",
    "        weights_hidden_dropped = tf.nn.dropout(weights_hidden, keep_prob)\n",
    "        biases_hidden = tf.Variable(tf.constant(0.1, shape=[1024]), name='biases')\n",
    "        relu_output = tf.nn.relu(tf.matmul(tf_train_dataset, weights_hidden_dropped) + biases_hidden)\n",
    "\n",
    "    with tf.name_scope('output'):\n",
    "        weights_output = tf.Variable(tf.truncated_normal([1024, num_labels], stddev=0.1), name='weights')\n",
    "        weights_output_dropped = tf.nn.dropout(weights_output, keep_prob)\n",
    "        biases_output = tf.Variable(tf.constant(0.1, shape=[num_labels]), name='biases')\n",
    "        logits = tf.matmul(relu_output, weights_output_dropped) + biases_output\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    loss = tf.reduce_mean( loss + beta * tf.nn.l2_loss(weights_output_dropped))\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(\n",
    "        tf.nn.relu(tf.matmul(\n",
    "            tf_valid_dataset, weights_hidden) + biases_hidden),\n",
    "        weights_output) + biases_output)\n",
    "\n",
    "    test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(\n",
    "       tf.nn.relu(tf.matmul(\n",
    "            tf_test_dataset, weights_hidden) + biases_hidden),\n",
    "        weights_output) + biases_output)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "for kp in np.arange(0.5,1,0.1):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size) % 10\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : batch_data, \n",
    "            tf_train_labels : batch_labels,\n",
    "            keep_prob: kp}\n",
    "        \n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], \n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "      print(\"Keep prob: %s Test accuracy: %.1f%%\" % (kp, accuracy(test_prediction.eval(), test_labels)))\n",
    "      accuracy_val_nn_l2.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy measures with different dropout tactics:\n",
    "\n",
    "Dropout on both layers results:\n",
    "\n",
    "    Initialized\n",
    "    Keep prob: 0.5 Test accuracy: 75.3%\n",
    "    Initialized\n",
    "    Keep prob: 0.6 Test accuracy: 76.7%\n",
    "    Initialized\n",
    "    Keep prob: 0.7 Test accuracy: 76.8%\n",
    "    Initialized\n",
    "    Keep prob: 0.8 Test accuracy: 76.4%\n",
    "    Initialized\n",
    "    Keep prob: 0.9 Test accuracy: 74.1%\n",
    "    \n",
    "Dropout on both layers plus l2 reguralization with a 0.0001 beta:\n",
    "\n",
    "    Initialized\n",
    "    Keep prob: 0.5 Test accuracy: 75.9%\n",
    "    Initialized\n",
    "    Keep prob: 0.6 Test accuracy: 76.0%\n",
    "    Initialized\n",
    "    Keep prob: 0.7 Test accuracy: 75.8%\n",
    "    Initialized\n",
    "    Keep prob: 0.8 Test accuracy: 75.3%\n",
    "    Initialized\n",
    "    Keep prob: 0.9 Test accuracy: 75.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Part\n",
    "\n",
    "Prepare data and variables for convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 28, 1) (55000, 10)\n",
      "(1000, 28, 28, 1) (1000, 10)\n",
      "(9000, 28, 28, 1) (9000, 10)\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 32\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "\n",
    "train_dataset_conv = reformat(train_dataset)\n",
    "valid_dataset_conv = reformat(valid_dataset)\n",
    "test_dataset_conv = reformat(test_dataset)\n",
    "\n",
    "print(train_dataset_conv.shape, train_labels.shape)\n",
    "print(valid_dataset_conv.shape, valid_labels.shape)\n",
    "print(test_dataset_conv.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convolutional network, stride=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth = 16\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "    tf_valid_dataset = tf.constant(valid_dataset_conv)\n",
    "    tf_test_dataset = tf.constant(test_dataset_conv)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], \n",
    "                                                    stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.475591\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 12.7%\n",
      "Minibatch loss at step 50: 2.185037\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 23.2%\n",
      "Minibatch loss at step 100: 0.858086\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 150: 0.733255\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 200: 1.191509\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 250: 0.284247\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 300: 0.651432\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 350: 0.048483\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 400: 0.254464\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 450: 0.304775\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 500: 0.541844\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 550: 0.112686\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 600: 0.279609\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 650: 0.304930\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 700: 0.304760\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 750: 0.513473\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 800: 0.563449\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 850: 0.105676\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 900: 0.188333\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 950: 0.826272\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 1000: 0.213254\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.6%\n",
      "Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset_conv[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffed up convolutional network\n",
    "- max pooling\n",
    "- dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "depth = 32\n",
    "graph = tf.Graph()\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Placeholders\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Input data.\n",
    "    tf_train_batch = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels))\n",
    "    # The None at the shape argument means that the dimension is not defined,\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "    # Constants\n",
    "    tf_valid_dataset = tf.constant(valid_dataset_conv)\n",
    "    tf_test_dataset = tf.constant(test_dataset_conv)\n",
    "\n",
    "    # Variables.\n",
    "    h_conv1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    h_conv1_biases = bias_variable([depth])\n",
    "\n",
    "    h_conv2_weights = weight_variable([patch_size, patch_size, depth, depth * 2])\n",
    "    h_conv2_biases = bias_variable([depth * 2])\n",
    "    \n",
    "    conv_image_size = image_size // 4\n",
    "    fc1_weights = weight_variable([conv_image_size * conv_image_size * depth * 2, num_hidden])\n",
    "    fc1_biases = bias_variable([num_hidden])\n",
    "\n",
    "    output_softmax_weights = weight_variable([num_hidden, num_labels])\n",
    "    output_softmax_biases = bias_variable([num_labels])\n",
    "    \n",
    "    #Define the model:\n",
    "    # First layer, patches of 5x5 into 32 features\n",
    "    h_conv1 = tf.nn.relu(conv2d(tf_train_batch, h_conv1_weights) + h_conv1_biases)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Second layer, patches of 5x5 into 64 features\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, h_conv2_weights) + h_conv2_biases)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Reshape into the densely connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, conv_image_size * conv_image_size * depth * 2])\n",
    "    \n",
    "    # Define the fully connected layer\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, fc1_weights) + fc1_biases)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Readout layer\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, output_softmax_weights) + output_softmax_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1875\n",
      "step 100, training accuracy 0.6875\n",
      "step 200, training accuracy 0.4375\n",
      "step 300, training accuracy 0.75\n",
      "step 400, training accuracy 0.875\n",
      "step 500, training accuracy 0.8125\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.5625\n",
      "step 800, training accuracy 0.6875\n",
      "step 900, training accuracy 0.9375\n",
      "step 1000, training accuracy 0.8125\n",
      "step 1100, training accuracy 0.9375\n",
      "step 1200, training accuracy 0.8125\n",
      "step 1300, training accuracy 0.875\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.9375\n",
      "step 1600, training accuracy 0.8125\n",
      "step 1700, training accuracy 0.9375\n",
      "step 1800, training accuracy 0.9375\n",
      "step 1900, training accuracy 0.875\n",
      "step 2000, training accuracy 1\n",
      "step 2100, training accuracy 0.8125\n",
      "step 2200, training accuracy 1\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 0.9375\n",
      "step 2600, training accuracy 1\n",
      "step 2700, training accuracy 1\n",
      "step 2800, training accuracy 0.9375\n",
      "step 2900, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "test accuracy 0.952111\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    # Training computation.\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf_train_labels * tf.log(y_conv), reduction_indices=[1]))\n",
    "\n",
    "    # Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    # These two lines are measure the accuracy of our model.\n",
    "    # y_conv is a softmax output, the highest entry is the most probable according to our model \n",
    "    # (e.g.: [0.7, 0.2, 0.5, 0.5])\n",
    "    # tf_train_labels are the original labels for the training set. \n",
    "    # (eg.: [0, 0, 0, 1])\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(tf_train_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Initialize the session variables.\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for step in range(3001):\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # I should randomize this part a bit more to reduce the possibility of reoccuring batches.\n",
    "        batch_data = train_dataset_conv[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={tf_train_batch:  batch_data, \n",
    "                                                      tf_train_labels: batch_labels, \n",
    "                                                      keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\" % (step, train_accuracy))\n",
    "        \n",
    "        train_step.run(feed_dict={tf_train_batch:  batch_data, \n",
    "                                  tf_train_labels: batch_labels, \n",
    "                                  keep_prob: 0.5})\n",
    "        \n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={tf_train_batch: test_dataset_conv,\n",
    "                                                        tf_train_labels: test_labels, \n",
    "                                                        keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 20'000 Iterations it has achieved a test accuracy of 0.986..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

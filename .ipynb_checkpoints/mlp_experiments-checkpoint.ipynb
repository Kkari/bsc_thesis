{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn.python.learn as learn\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import six.moves.cPickle as pickle\n",
    "import sys\n",
    "from pandas import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(name, var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset, train_labels, test_dataset, test_labels, train_tensor,\n",
    "                accuracy, tf_batch_data, tf_batch_labels, log_dir='./logs',\n",
    "                num_steps=20000, batch_size=10, test_steps=1000, log_steps=100):\n",
    "    with tf.Session() as session:\n",
    "        summaries = tf.merge_all_summaries()\n",
    "        train_writer = tf.train.SummaryWriter(log_dir + '/train', session.graph)\n",
    "        test_writer = tf.train.SummaryWriter(log_dir + '/test')\n",
    "\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        shuffle_train = np.random.permutation(train_dataset.shape[0])\n",
    "        train_dataset = train_dataset[shuffle_train]\n",
    "        train_labels = train_labels[shuffle_train]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = ((step * batch_size) % (train_labels.shape[0] - batch_size))\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {\n",
    "                tf_batch_data : batch_data, \n",
    "                tf_batch_labels : batch_labels\n",
    "            }\n",
    "    \n",
    "    \n",
    "            if step % test_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, acc, summary = session.run([train_tensor, accuracy, summaries], \n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata,\n",
    "                                             options=run_options)\n",
    "                print(\"Train accuracy at step %s: %.1f%%\" % (step, acc * 100))\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                \n",
    "            elif step % log_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, summary = session.run([train_tensor, summaries], \n",
    "                                         feed_dict=feed_dict, \n",
    "                                         run_metadata=run_metadata,\n",
    "                                         options=run_options)\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                session.run(train_tensor, feed_dict=feed_dict, options=run_options)\n",
    "\n",
    "\n",
    "        feed_dict = {\n",
    "            tf_batch_data : test_dataset, \n",
    "            tf_batch_labels : test_labels\n",
    "        }\n",
    "        acc = session.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Train accuracy: %.3f%%\" % acc * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at step 0: 0.1%\n",
      "Train accuracy at step 1000: 1.0%\n",
      "Train accuracy at step 2000: 1.0%\n",
      "Train accuracy at step 3000: 1.0%\n",
      "Train accuracy at step 4000: 1.0%\n",
      "Train accuracy: 0.990%\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(batch_data):\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "\n",
    "        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')\n",
    "        variable_summaries('fc/fc_1', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 400, activation_fn=None, scope='fc/fc_2')\n",
    "#         variable_summaries('fc/fc_2', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 1024, scope='fc/fc_6')\n",
    "#         variable_summaries('fc/fc_6', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 400, activation_fn=None, scope='fc/fc_3')\n",
    "#         variable_summaries('fc/fc_3', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 1024, scope='fc/fc_4')\n",
    "#         variable_summaries('fc/fc_4', x)\n",
    "       \n",
    "        x = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_5')\n",
    "        variable_summaries('fc/fc_5', x)\n",
    "#         x = slim.fully_connected(x, 10, activation_fn=None ,scope='fc/fc_3')\n",
    "        predictions = tf.nn.softmax(x)\n",
    "        return x, predictions\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    last_layer, predictions = fully_connected(batch_data)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=mnist.train.images, \n",
    "                train_labels=mnist.train.labels, \n",
    "                test_dataset=mnist.train.images,\n",
    "                test_labels=mnist.train.labels, \n",
    "                train_tensor=train_tensor, accuracy=accuracy,\n",
    "                tf_batch_data=batch_data, tf_batch_labels=batch_labels, batch_size=128, num_steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fully connected results:\n",
    "* 768-10: 92%\n",
    "* 768-1024-10: 96,1%\n",
    "* 768-400-10: 99,4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.reshape([55000, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = np.array([[1,2,3,4],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]]],\n",
       "\n",
       "\n",
       "       [[[5],\n",
       "         [6]],\n",
       "\n",
       "        [[7],\n",
       "         [8]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narr = np.expand_dims(array.reshape([2,2,2]), axis=3)\n",
    "narr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(mnist.train.images.reshape([-1,28,28]), axis=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db661c64f22d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcifar_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-10-batches-py/test_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m cifar['data_3d'] = np.dstack((cifar_test['data'][:,0:1024].reshape(32,32),\n\u001b[1;32m      9\u001b[0m                              \u001b[0mcifar_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-db661c64f22d>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin-1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def from_flat_to_3d(image):\n",
    "    return np.dstack((image['data'][:,0:1024].reshape(32,32),\n",
    "               image['data'][1024:2048].reshape(32,32),\n",
    "               image['data'][2048:3072].reshape(32,32)))\n",
    "\n",
    "cifar_test = unpickle('cifar-10-batches-py/test_batch')\n",
    "cifar_test['data_3d'] = [from_flat_to_3d(image) for image in cifar_test['data']]\n",
    "\n",
    "cifar = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "for i in range(2, 6):\n",
    "    tmp = unpickle('cifar-10-batches-py/data_batch_' + str(i))\n",
    "    cifar['data'] = np.vstack((cifar['data'], tmp['data']))\n",
    "    cifar['labels'] = np.concatenate((cifar['labels'], tmp['labels']))\n",
    "\n",
    "cifar['data_3d'] = np.dstack((cifar['data'][:,0:1024].reshape(32,32),\n",
    "                             cifar['data'][1024:2048].reshape(32,32),\n",
    "                             cifar['data'][2048:3072].reshape(32,32)))\n",
    "\n",
    "cifar['data_bw'] = (cifar['data'][:,0:1024] + cifar['data'][:,1024:2048] + cifar['data'][:, 2048:3072]) / 3 \n",
    "cifar_test['data_bw'] = (cifar_test['data'][:,0:1024] + cifar_test['data'][:,1024:2048] + cifar_test['data'][:, 2048:3072]) / 3 \n",
    "\n",
    "enc = OneHotEncoder()\n",
    "cifar['labels_oh'] = enc.fit_transform(cifar['labels'].reshape(-1, 1))\n",
    "cifar['labels_oh'] = cifar['labels_oh'].toarray()\n",
    "\n",
    "cifar_test['labels'] = np.array(cifar_test['labels'])\n",
    "cifar_test['labels_oh'] = enc.fit_transform(cifar_test['labels'].reshape(-1, 1))\n",
    "cifar_test['labels_oh'] = cifar_test['labels_oh'].toarray()\n",
    "\n",
    "pca = PCA(whiten=True)\n",
    "cifar['data_bw_whitened'] = pca.fit_transform(cifar['data_bw'])\n",
    "cifar_test['data_bw_whitened'] = pca.fit_transform(cifar_test['data_bw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(cifar['data'], cifar['labels'])\n",
    "lr.predict(cifar['data'])\n",
    "print(lr.score(cifra_test['data'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data_bw'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data_bw'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data_bw_whitened'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data_bw_whitened'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at step 0: 0.2%\n",
      "Train accuracy at step 1000: 0.2%\n",
      "Train accuracy at step 2000: 0.4%\n",
      "Train accuracy at step 3000: 0.2%\n",
      "Train accuracy at step 4000: 0.3%\n",
      "Train accuracy at step 5000: 0.3%\n",
      "Train accuracy at step 6000: 0.3%\n",
      "Train accuracy at step 7000: 0.3%\n",
      "Train accuracy at step 8000: 0.2%\n",
      "Train accuracy at step 9000: 0.1%\n",
      "Train accuracy at step 10000: 0.2%\n",
      "Train accuracy at step 11000: 0.2%\n",
      "Train accuracy at step 12000: 0.2%\n",
      "Train accuracy at step 13000: 0.3%\n",
      "Train accuracy at step 14000: 0.3%\n",
      "Train accuracy at step 15000: 0.2%\n",
      "Train accuracy at step 16000: 0.3%\n",
      "Train accuracy at step 17000: 0.2%\n",
      "Train accuracy at step 18000: 0.3%\n",
      "Train accuracy at step 19000: 0.3%\n",
      "Train accuracy: 0.215%\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(batch_data):\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        # First Layer\n",
    "        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')\n",
    "        variable_summaries('fc/fc_1', x)\n",
    "        \n",
    "        # Second Layer\n",
    "        x = slim.fully_connected(x, 1024, scope='fc/fc_2')\n",
    "        variable_summaries('fc/fc_2', x)\n",
    "        \n",
    "        # Third Layer\n",
    "        x = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_3')\n",
    "        variable_summaries('fc/fc_3', x)\n",
    "        predictions = tf.nn.softmax(x)\n",
    "        return x, predictions\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, 32 * 32 * 3))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    last_layer, predictions = fully_connected(batch_data)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=cifar['data'], \n",
    "                train_labels=cifar['labels_oh'], \n",
    "                test_dataset=cifar_test['data'],\n",
    "                test_labels=cifar_test['labels_oh'], \n",
    "                train_tensor=train_tensor, accuracy=accuracy,\n",
    "#                 log_dir='cifar_mlp_400_10',\n",
    "                tf_batch_data=batch_data, tf_batch_labels=batch_labels, batch_size=128, num_steps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cifar-10 mlp results:**\n",
    "* Sokkal jobb processzor kihasználás mint az sklearn-nél!\n",
    "* 3072-10, rate=0.5, steps=20k: 20.7%\n",
    "* 3072-10, rate=0.001, steps=20k: 25%\n",
    "* 1024-10, rate=0.001, steps=20k, bw_w: 9.7%\n",
    "* 1024-10, rate=0.001, steps=20k, bw: 10.3%\n",
    "* 3072-400-10 rate=0.001, steps=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

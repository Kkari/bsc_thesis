{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn.python.learn as learn\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import six.moves.cPickle as pickle\n",
    "import sys\n",
    "from pandas import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(name, var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(train_dataset, train_labels, test_dataset, test_labels, train_tensor,\n",
    "                accuracy, tf_batch_data, tf_batch_labels, log_dir='./logs',\n",
    "                num_steps=20000, batch_size=10, test_steps=1000, log_steps=100):\n",
    "    with tf.Session() as session:\n",
    "        summaries = tf.merge_all_summaries()\n",
    "        import shutil\n",
    "        \n",
    "        if tf.gfile.Exists(log_dir):\n",
    "            tf.gfile.DeleteRecursively(log_dir)\n",
    "            \n",
    "        train_writer = tf.train.SummaryWriter(log_dir + '/train', session.graph)\n",
    "        test_writer = tf.train.SummaryWriter(log_dir + '/test')\n",
    "\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        shuffle_train = np.random.permutation(train_dataset.shape[0])\n",
    "        train_dataset = train_dataset[shuffle_train]\n",
    "        train_labels = train_labels[shuffle_train]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = ((step * batch_size) % (train_labels.shape[0] - batch_size))\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size)]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {\n",
    "                tf_batch_data : batch_data, \n",
    "                tf_batch_labels : batch_labels\n",
    "            }\n",
    "    \n",
    "    \n",
    "            if step % test_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, acc, summary = session.run([train_tensor, accuracy, summaries], \n",
    "                                             feed_dict=feed_dict,\n",
    "                                             run_metadata=run_metadata,\n",
    "                                             options=run_options)\n",
    "                print(\"Train accuracy at step %s: %.1f%%\" % (step, acc * 100))\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                \n",
    "            elif step % log_steps == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, summary = session.run([train_tensor, summaries], \n",
    "                                         feed_dict=feed_dict, \n",
    "                                         run_metadata=run_metadata,\n",
    "                                         options=run_options)\n",
    "                train_writer.add_run_metadata(run_metadata, \"step%d\" % step)\n",
    "                train_writer.add_summary(summary, step)\n",
    "            else:\n",
    "                session.run(train_tensor, feed_dict=feed_dict, options=run_options)\n",
    "\n",
    "\n",
    "        feed_dict = {\n",
    "            tf_batch_data : test_dataset, \n",
    "            tf_batch_labels : test_labels\n",
    "        }\n",
    "        acc = session.run(accuracy, feed_dict=feed_dict)\n",
    "        print(\"Train accuracy: %.3f%%\" % (acc * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at step 0: 7.8%\n",
      "Train accuracy at step 1000: 93.0%\n",
      "Train accuracy at step 2000: 98.4%\n",
      "Train accuracy at step 3000: 98.4%\n",
      "Train accuracy at step 4000: 99.2%\n",
      "Train accuracy at step 5000: 100.0%\n",
      "Train accuracy at step 6000: 98.4%\n",
      "Train accuracy at step 7000: 99.2%\n",
      "Train accuracy at step 8000: 99.2%\n",
      "Train accuracy at step 9000: 97.7%\n",
      "Train accuracy at step 10000: 98.4%\n",
      "Train accuracy at step 11000: 98.4%\n",
      "Train accuracy at step 12000: 99.2%\n",
      "Train accuracy at step 13000: 99.2%\n",
      "Train accuracy at step 14000: 99.2%\n",
      "Train accuracy at step 15000: 99.2%\n",
      "Train accuracy at step 16000: 98.4%\n",
      "Train accuracy at step 17000: 98.4%\n",
      "Train accuracy at step 18000: 99.2%\n",
      "Train accuracy at step 19000: 99.2%\n",
      "Train accuracy: 99.100%\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(batch_data):\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "\n",
    "        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')\n",
    "        variable_summaries('fc/fc_1', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 400, activation_fn=None, scope='fc/fc_2')\n",
    "#         variable_summaries('fc/fc_2', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 1024, scope='fc/fc_6')\n",
    "#         variable_summaries('fc/fc_6', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 400, activation_fn=None, scope='fc/fc_3')\n",
    "#         variable_summaries('fc/fc_3', x)\n",
    "        \n",
    "#         x = slim.fully_connected(x, 1024, scope='fc/fc_4')\n",
    "#         variable_summaries('fc/fc_4', x)\n",
    "       \n",
    "        x = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_2')\n",
    "        variable_summaries('fc/fc_2', x)\n",
    "#         x = slim.fully_connected(x, 10, activation_fn=None ,scope='fc/fc_3')\n",
    "        predictions = tf.nn.softmax(x)\n",
    "        return x, predictions\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, image_size * image_size))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    last_layer, predictions = fully_connected(batch_data)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=mnist.train.images, \n",
    "                train_labels=mnist.train.labels, \n",
    "                test_dataset=mnist.train.images,\n",
    "                test_labels=mnist.train.labels, \n",
    "                train_tensor=train_tensor, accuracy=accuracy,\n",
    "                log_dir='mlp_adams',\n",
    "                tf_batch_data=batch_data, tf_batch_labels=batch_labels, batch_size=128, num_steps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Fully connected results:\n",
    "* 768-10: 92%\n",
    "* 768-1024-10: 96,1%\n",
    "* 768-400-10: 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.reshape([55000, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = np.array([[1,2,3,4],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]]],\n",
       "\n",
       "\n",
       "       [[[5],\n",
       "         [6]],\n",
       "\n",
       "        [[7],\n",
       "         [8]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narr = np.expand_dims(array.reshape([2,2,2]), axis=3)\n",
    "narr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(mnist.train.images.reshape([-1,28,28]), axis=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin-1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def from_flat_to_3d(image):\n",
    "#     print(image.shape)\n",
    "    return np.dstack((image[0:1024].reshape(32,32),\n",
    "                       image[1024:2048].reshape(32,32),\n",
    "                       image[2048:3072].reshape(32,32)))\n",
    "\n",
    "cifar_test = unpickle('cifar-10-batches-py/test_batch')\n",
    "cifar_test['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar_test['data']])\n",
    "\n",
    "cifar = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "for i in range(2, 6):\n",
    "    tmp = unpickle('cifar-10-batches-py/data_batch_' + str(i))\n",
    "    cifar['data'] = np.vstack((cifar['data'], tmp['data']))\n",
    "    cifar['labels'] = np.concatenate((cifar['labels'], tmp['labels']))\n",
    "\n",
    "cifar['data_3d'] = np.array([from_flat_to_3d(image) for image in cifar['data']])\n",
    "\n",
    "cifar['data_bw'] = (cifar['data'][:,0:1024] + cifar['data'][:,1024:2048] + cifar['data'][:, 2048:3072]) / 3 \n",
    "cifar_test['data_bw'] = (cifar_test['data'][:,0:1024] + cifar_test['data'][:,1024:2048] + cifar_test['data'][:, 2048:3072]) / 3 \n",
    "\n",
    "enc = OneHotEncoder()\n",
    "cifar['labels_oh'] = enc.fit_transform(cifar['labels'].reshape(-1, 1))\n",
    "cifar['labels_oh'] = cifar['labels_oh'].toarray()\n",
    "\n",
    "cifar_test['labels'] = np.array(cifar_test['labels'])\n",
    "cifar_test['labels_oh'] = enc.fit_transform(cifar_test['labels'].reshape(-1, 1))\n",
    "cifar_test['labels_oh'] = cifar_test['labels_oh'].toarray()\n",
    "\n",
    "pca = PCA(whiten=True)\n",
    "cifar['data_bw_whitened'] = pca.fit_transform(cifar['data_bw'])\n",
    "cifar_test['data_bw_whitened'] = pca.fit_transform(cifar_test['data_bw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkari/DevTools/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cifra_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c4f43a7ffcb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifra_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cifra_test' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag')\n",
    "lr.fit(cifar['data'], cifar['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_test['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3886\n"
     ]
    }
   ],
   "source": [
    "print(lr.score(cifar_test['data'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data_bw'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data_bw'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data_bw_whitened'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data_bw_whitened'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_raw = svm.SVC(decision_function_shape='ovr', kernel='poly')\n",
    "clf_raw.fit(cifar['data'], cifar['labels'])\n",
    "print(svm.score(cifar_test['data'], cifar_test['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy at step 0: 0.2%\n",
      "Train accuracy at step 1000: 0.2%\n",
      "Train accuracy at step 2000: 0.4%\n",
      "Train accuracy at step 3000: 0.2%\n",
      "Train accuracy at step 4000: 0.3%\n",
      "Train accuracy at step 5000: 0.3%\n",
      "Train accuracy at step 6000: 0.3%\n",
      "Train accuracy at step 7000: 0.3%\n",
      "Train accuracy at step 8000: 0.2%\n",
      "Train accuracy at step 9000: 0.1%\n",
      "Train accuracy at step 10000: 0.2%\n",
      "Train accuracy at step 11000: 0.2%\n",
      "Train accuracy at step 12000: 0.2%\n",
      "Train accuracy at step 13000: 0.3%\n",
      "Train accuracy at step 14000: 0.3%\n",
      "Train accuracy at step 15000: 0.2%\n",
      "Train accuracy at step 16000: 0.3%\n",
      "Train accuracy at step 17000: 0.2%\n",
      "Train accuracy at step 18000: 0.3%\n",
      "Train accuracy at step 19000: 0.3%\n",
      "Train accuracy: 0.215%\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(batch_data):\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        # First Layer\n",
    "        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')\n",
    "        variable_summaries('fc/fc_1', x)\n",
    "        \n",
    "        # Second Layer\n",
    "        x = slim.fully_connected(x, 1024, scope='fc/fc_2')\n",
    "        variable_summaries('fc/fc_2', x)\n",
    "        \n",
    "        # Third Layer\n",
    "        x = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_3')\n",
    "        variable_summaries('fc/fc_3', x)\n",
    "        predictions = tf.nn.softmax(x)\n",
    "        return x, predictions\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    batch_data = tf.placeholder(tf.float32, shape=(None, 32 * 32 * 3))\n",
    "    batch_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "    last_layer, predictions = fully_connected(batch_data)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(last_layer, batch_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "    tf.scalar_summary('losses/total_loss', total_loss)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions,1), tf.argmax(batch_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "\n",
    "    train_model(train_dataset=cifar['data'], \n",
    "                train_labels=cifar['labels_oh'], \n",
    "                test_dataset=cifar_test['data'],\n",
    "                test_labels=cifar_test['labels_oh'], \n",
    "                train_tensor=train_tensor, accuracy=accuracy,\n",
    "#                 log_dir='cifar_mlp_400_10',\n",
    "                tf_batch_data=batch_data, tf_batch_labels=batch_labels, batch_size=128, num_steps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cifar-10 mlp results:**\n",
    "* Sokkal jobb processzor kihasználás mint az sklearn-nél!\n",
    "* 3072-10, rate=0.5, steps=20k: 20.7%\n",
    "* 3072-10, rate=0.001, steps=20k: 25%\n",
    "* 1024-10, rate=0.001, steps=20k, bw_w: 9.7%\n",
    "* 1024-10, rate=0.001, steps=20k, bw: 10.3%\n",
    "* 3072-400-10 rate=0.001, steps=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

%----------------------------------------------------------------------------
\chapter{A Neurális hálózatokkal való képosztályozás lehetõségeinek áttekintése}
%----------------------------------------------------------------------------

A kutatómunka jelentõs részét tette ki a dolgozatomnak, mivel az évek folyamán nagyon sok sikeres és sikertelen kísérlet született annak érdekében hogy hogyan lehetne neurális hálózatokkal képeket osztályozni. Talán mondhatjuk hogy ezek a hálózatok a legsikeresebb képosztályozó eljárások, de korántsem triviális hogy mik az elõnyeik, hátrányaik és az egyes típusok milyen komplexitású jelekkel képesek megbírkózni. Elõször szeretném bemutatni az adathalmazokat amiken ezeket az algoritmusokat kiértékelik, majd eljutni az többrétegû perceptronoktól a konvoluciós hálózatokig, végül egy-két újabb trend ismertetésével zárni a fejezetet.

\section{Az algoritmusok kiértékelése}

\subsection{Az adathalmazok}
\paragraph{Bevezetés} Mint a legtöbb kutatási területnek, ennek is vannak jól ismert "benchmark" adathalmazai, amelyek viszonyítási alapként lehetõséget biztosítanak az egymástól eltérõ algoritmusok egymáshoz képesti kiértékelésére. Mivel ezekre az adathalmazokra sokat fogok hivatkozni, ezért szeretném õket egy-egy bekezdésben bemutatni.

\paragraph{MNIST\cite{mnist}} Az MNIST adatbázis fekete-fehér, 28x28 pixelre normalizált írott számjegyeket tartalmaz nullától kilencig, azaz tíz osztállyal rendelkezik. Az adatbázis 60'000 annotált tanító képet és 10'000 annotált teszt képet tartalmaz. Ez a legalapabb adathalmaz. Az adathalmazból a \figref{mnistSample}~ábra mutat egy-két példát. 

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/mnistSample.png}
\caption{Példa az MNIST adathalmaz képeire. Forrás: \url{http://knowm.org/wp-content/uploads/Screen-Shot-2015-08-14-at-2.44.57-PM.png}.} 
\label{fig:mnistSample}
\end{figure}

\paragraph{CIFAR-10\cite{cifar}} A CIFAR-10 egy jóval összetettebb adathalmaz, 60'000 annotált 32x32 pixeles, színes képet tartalmaz. A képek 10 osztályra vannak felosztva, osztályonként 6000 képpel. Az adathalmazból 50'000 kép van tanításra, és 10'000 tesztelésre fenntartva. Az adathalmazból a \figref{cifarSample}~ábra mutat egy-két példát.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/cifarSample.png}
\caption{Példa a CIFAR-10/100 adathalmaz képeire. Forrás:\url{http://www.cs.toronto.edu/~kriz/cifar.html}} 
\label{fig:cifarSample}
\end{figure}

\paragraph{CIFAR-100\cite{cifar}} A CIFAR-100 felépítése megegyezik a CIFAR-10el, de osztályrendszere az elõbbinél lényegesen összetettebb, 100 osztályt tartalmaz és minden osztályhoz 600 kép tartozik, ezen felül még 20 általánosabb osztályba is be vannak sorolva a képek, hogy a hálózat általánosításáról következtetéseket lehessen levonni. Például az halak szuperosztályhoz tartozik a rája, lazac, stb. Példának úgyanúgy a CIFAR-10 mintája, a \figref{cifarSample}~ábra tekinthetõ.

\paragraph{IMAGENET\cite{imagenet}} Az IMAGENET a világ legnagyobb képgyüjteménye, a WordNet lexikális adatbázis szinoníma halmazai szerint vannak annotálva a képek. Jelenlegi statiszikái:
\begin{itemize}
\item 14'197'112 annotált kép
\item 21'841 nem üres szinoníma halmaz
\item 1'034'908 kép objektumaihoz van még határoló doboz annotáció is
\item 1'000 szinoníma halmazhoz tartozik SIFT jellemzõkkel ellátott kép
\item 1'200'000 kép van SIFT jellemzõkkel ellátva.
\end{itemize}
Látható hogy az elõzõ három adathalmazt az IMAGENET már csak puszta méreteivel is messze túlszárnyalja. Ezt mondhatjuk az etalon benchmarknak. Az évente megrendezett, a gépi látás "olimpiájának" számító ILSVRC(Large Scale Visual Recognition Challenge) is ezen az adatsokaságon szokott megrendezésre kerülni, jellemzõen 4 kategóriában: objektum lokalizáció, objektum detekció, helyszín felismerés (pl tengerpart, hegyek), helyszín megértés. A legutóbbi nem takar kevesebbet mint egy kép szemantikus részekre való felosztása, példáúl út, ég, ember vagy ágy. A \figref{imagenetSample}~ábra ebbõl az adathalmazból mutat pár példát.

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/imagenetSample.png}
\caption{Példa az IMAGENET adathalmaz képeire. Forrás: \url{http://5047-presscdn.pagely.netdna-cdn.com/wp-content/uploads/2014/12/imagenet.jpg}.} 
\label{fig:imagenetSample}
\end{figure}

\subsubsection{A metrika}
\paragraph{MNIST és CIFAR} Az MNIST és a CIFAR-10/100 Adathalmazok esetében mindig az egyszerû pontosság értéket nézzük, tehát az eltalált képek számát osztva a hibásan osztályozott képet számával.

\paragraph{IMAGENET} Mivel az IMAGENET egy ennyire bonyolult adathalmaz, itt top 1 és top 5 hibát is szoktak nézni. A top 5 hiba ahol az számít sikeres találatnak ha a helyes címkét a háló 5 legnagyobb valószínûséggel bíró tippjében benne van.

\section{A Legelterjedtebb neurális hálózatok képfeldolgozáshoz}

\paragraph{Bevezetés} Az évek során számos neurális hálózattal kísérleteztek a kutatók annak érdekében hogy rájöjjenek melyek képesek legjobban megtanulni a képekeben elõforduló szabályosságokat, és ez alapján osztályozni õket. Ezekbõl szeretném a fõ állomásokat kiemelni, és leírni hogy mik voltak a hiányosságok a meglévõ architektúrákban amik új struktúrák létrehozását motiválták. Az áttekintésben nem ejtek szót a minden hálózat típusra érvényes általános, különféle reguralizációs eljárásokról, mint a súlyok felejtése vagy a dropout metódus. Csak a hálózatok felépítésének és tanításának architektúrális különbségeit veszem górcsõ alá.

\subsection{A többrétegû perceptron (MLP)}

\subsubsection{Az MLP elõzményei és megalkotása}
\paragraph{Elõzmények} Miután Rosenblatt megalkotta az elsõ perceptron struktúrát a kései ötvenes években, a kutatók elkezdtek azon gondolkodni hogy hogyan lehetne ezeket a neuronokat összerendezni úgy, hogy együtt tanuljanak, és komplex regressziók, illetve osztályozási feladatok elvégzésére legyenek képesek. De ezek a kutatások sokáig igen meddõek voltak.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/MLP.png}
\caption{Egy kétrétegû MLP hálózat. Forrás:\url{https://mialmanach.mit.bme.hu/neuralis/ch04s01}}
\label{fig:MLP}
\end{figure}

\paragraph{Backpropagation} Az áttörés 1986-ban jött, amikor Geoffrey E. Hinton kollégáival sikeresen alkalmazta a hibavisszaterjesztéses algoritmust\cite{backProp} az MLP súlyainak megváltoztatására a négyzetes hiba minimalizálásának érdekében.
Az algoritmus lényege hogy a hibát a hálózatban a deriválás lánc szabályának segítségével terjesztjük vissza. Az algoritmust helymegtakarítás érdekében részletesebben nem ismertetem, az érdeklõdõk a bekezdés elején referált cikkben további részleteket találhatnak. Az algoritmus pseudokód összefoglalását a \figref{MLP}-ábrán 
látható hálózat tanításához a \listref{Backpropagation}-lista mutatja.

\begin{lstlisting}[frame=single,float=!ht,caption= A backpropagation algoritmus pseudokódja, label=listing:Backpropagation, mathescape=true]
  inicializáljuk a háló súlyait (általában 0-1 közé esõ véletlen számok)
  do
     forEach tanító példa legyen tp
        jósolt_címke = háló-kiement(háló, tp)
        valódi_címke = tanító_címke(ex)
        hiba számítás f(jósolt_címke - valódi_címke) minden kimeneti egységen
        $\Delta W^{(2)}$ kiszámítása
        $\Delta W^{(1)}$ kiszámítása
        a hálózat súlyainak frissítése
  until Az összes bemenet sikeresen van osztályozva, 
  		vagy más megállási kritériumot el nem értünk
  return a hálózatot
\end{lstlisting}

\subsubsection{Az MLP teljesítménye képosztályozási feladatokra}
\paragraph{Aktivítás a területen.} Gondolhatnánk hogy ezt a témát már rég elfelejtették a kutatók, de mivel az MLP egy igen egyszerû struktúra, ezért folyik még néhány kutatás hogy a határait megtalálják.

\paragraph{MNIST} Az MLP teljesítménye képosztályozási feladatok tekitetében igen szerény a többi hálózathoz képest, de az MNIST adathalmazzal még ez is egész jól megbírkózik, néhány figyelemre méltóbb eredményt a \tabref{MLPTablazat}~táblázat foglal össze. A többi adathalmazon a naiv MLP nem hoz értékelhetõ eredményt, ennek az okait mindjárt megvizsgáljuk.

\begin{table}[ht]
	\footnotesize
	\centering
	\caption{Az MLP teljesítménye az MNIST adathalmazon} \label{tab:MLPTablazat}
	\begin{tabular}{ | l | c | c |}
	\hline
	Rétegek száma & neuron struktúra & Teszt szet hiba százalék \\ \hline
	2-réteg & 300-10 & 4.7 \\
	2-réteg & 800-10 & 1.6 \\
	\hline
	\end{tabular}
\end{table}
Az MLP hálózatoknál jelentõs javulást hozott amikor a hibafüggvényt négyzetes középértékrõl kereszt-entrópiára cserélték ki. Ma már osztályozási feladatoknál szinte csak a kereszt-entrópia hibafüggvényt használjuk.

\subsubsection{MLP hiányosságai} 

\paragraph{A tanulás jellege} A felügyelt tanulás kapzsi módon a hibafüggvényt a súlyok gradiensének irányába optimalizálja, ezzel az a probléma hogy a hibafelület egy MLP esetében többé nem konvex mint egy egyszerû neuron esetében. A bonyolult hibafûggvény következtében lokális minimumok alakulnak ki. Sok esetben egy jó lokális minimumot sem érünk el naiv tanítással, a globális minimum elérésének az esélye pedig statisztikailag nulla. Ezt a jelenséget hivatott az alább egyszerû függvény $\sigma^2(\sigma(x) + \sigma(y))$ kontúr diagrammja (\figref{sigmaContour}~ábra). Ez habár nem közvetlenül egy hibafüggvény, de azt a tulajdonságot jól szemlélteti, hogy több fennsík alakúl ki, ahol a derivált 0 lesz. 

\paragraph{A struktúra kialakítása} Az MLP annyira általános struktúrát használ, hogy lényegében semmilyen a priori tudást nem használunk fel a hálózat tanításakor. Ez azt eredményezi hogy hatalmas kapacítás kell a képekben megjelenõ bonyolult struktúrák megtanulásához, és a hálózat különbözõ részei kénytelenek rendre ugyanazt megtanulni. Sajnos az elõbbi cél csak a hálózat növelésével érhetõ el, az MLP paraméter tere viszont nagyon rosszúl skálázódik. Ha veszünk egy 6 rétegû hálózatot aminek a rétegjei rendre 2500-2000-1500-1000-500-10 neuronból állnak, és az MNIST esetén 784 elemû bemeneti vektorral rendelkezik, akkor optimalizálandó paraméter tér mérete annak folytán hogy minden réteg teljesen össze van kapcsolva már:
$784 * 2500 + 2500 * 2000 + 2000 * 1500 + 1500 * 1000 + 1000 * 500 + 500 * 10 = 11 965 000$, ami már nyilvánvalóan hatalmas. Ez is tanítható sikeresen, de ahhoz már a továbbiakban bemutott kisegítõ neurális struktúrák szükségesek.

\begin{figure}[!ht]
\centering
\includegraphics[width=30mm, keepaspectratio]{figures/sigmaContour}
\caption{Példa egy komponált szigmoid függvényre.} 
\label{fig:sigmaContour}
\end{figure}

\paragraph{konkluzió}
Jól látszik hogy az MLP hálózatok ilyen naív formájukban nem igazán felelnek meg az elvárásainknak, ezt a késõbbiekben bemutatott mérési eredményeim meg is erõsítik. A továbbiakban bemutatok egy kisegítõ neurális struktúrát, amivel sikeresen inicializálták az MLP súlyait hogy jobb eredményeket érjenek el.

\subsection{A Korlátozott Boltzmann Gép}
\paragraph{Bevezetés} A korlátozott boltzmann gép (továbbiakban RBM - Restricted Boltzmann Machine, \figref{rbm}~ábra) egy szochasztikus generatív neurális számítási modell. Mûködése az eddig tárgyalt MLP-tõl györekesen eltér, funkciója a bemenet jellemzõinek (featureinek) a megtanulása, és nem az egyes minták helyes osztályozása. Az intuitív jelentõsége abban rejlik hogy feltehetjük hogy a naív MLP a kapzsi tanulása miatt nem képes megtanulni a minták valódi reprezentációját, de ha az MLP súlyait úgy tudnánk inicializálni, hogy a mintákban lévõ szabályosságokat már eleve ismerje, akkor ebbõl könnyebben meg tudná tanulni hogy melyik jellemzõ mely osztályt azonsítja.
Ezt felügyelt tanulás elõtti fázist nem felügyelt elõtanulásnak hívjuk. Szokás még erre a célra Autoencoder hálózatokat használni, illetve mélyebb hálókra az RBM és az autoencoder többrétegû megfelelõit a mély hiedelem hálózatokat, és a stacked autoencodereket.
Habár a legújabb eredmények szerint az Autoencoder hasonlóan jó eredményre vezet, és kevésbé bonyolult ezért gyakorlatban az ajánlott, én mégis az RBM-et választottam érdekes struktúrája miatt. Hugo Larochelle et al. hozott ki egy áttekintõ tanulmányt az elõtanulásról a mélyebben érdeklõdõknek "Exploring Strategies for Training Deep Neural Networks" \cite{exploringStrategies} címmel.

\paragraph{Az RBM struktúrája} Az RBM mint említettem egy sztochasztikus generatív számítási modell, amelyben fontos hogy az egyes neuronok egy páros gráfot alkotnak (\figref{rbm}~ábra). A generatív modell egy régi, statisztikából származó fogalom ami azt foglalja magában hogy a model képes megfigyelhetõ adatpontokat véletlenszerûen generálni. Az RBM egyik legtriviálisabb mérõszáma a rekonstrukciós hiba azt méri hogy ha egy adatpontot a háló bemenetére teszek, akkor azt milyen részletesen tudja visszagenerálni. Tehát az adatpont az a háló tanulási terében egy stabil pontnak számít-e. Ez a hiba mérték nem jó az RBM általánosító képességének mérésére, mégis sokan használják praktikus egyszerûsége miatt. Akit bõvebben érdekel a téma a "A practical guide to training restricted boltzmann machines"\cite{rbmGuide} referenciában talál bõséges irodalmat az RBM tanítását illetõen. Itt csak az alapokra szorítkozok.

\begin{figure}[!ht]
\centering
\includegraphics[width=50mm, keepaspectratio]{figures/rbm}
\caption{Egy RBM hálózat. Forrás: \protect\url{http://deeplearning.net/tutorial/_images/rbm.png}} 
\label{fig:rbm}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=50mm, keepaspectratio]{figures/rbmFilters}
\caption{Egy RBM által megtanult filterek. Forrás: \protect\url{http://www.pyimagesearch.com/wp-content/uploads/2014/06/rbm_filters.png}} 
\label{fig:rbmFilters}
\end{figure}

\paragraph{Az RBM tanítása} Ebben a bekezdésben belemegyek kicsit az RBM tanításának matematikájába, mert az implementáció tárgyalásánál fontos lesz. Az RBM azért érdekes megközelítés a többi hálózathoz képest, mert probablisztikus alapokon nyugszik. Az úgynevezett energia alapú hálózatok felfoghatóak úgy, hogy a hálózat minden konfigurációjához tartozik egy $p(x)$ valószínûség, hogy mekkora valõszínûséggel tartózkodik a háló az adott konfigurációban. Azt szeretnénk elérni hogy az alacsony energiájú konfigurációknak nagy legyen a valószínûsége. Ez formalizálva a következõ képpen néz ki:

\begin{align} \label{eq:probFirst}
p(x) = \frac{e^{(-E(x))}}{Z} = \sum_{h}\frac{e^{(-E(x,h))}}{Z}
\end{align}
\begin{align}
Z = \sum_{x} e^{-E(x)}
\end{align}

Ahol az E az energiafüggvényt, Z pedig a particiós függvényt jelenti. A fizikában jártasabb olvasók megfigyelhetik hogy ez a valószínûségi függvény megfelel a termodinamikában használt Boltzmann eloszlás valószínûségi függvényének. Az eredeti boltzmann gépet egy fizikus alkotta meg, pont erre az analógiára építve. A cél az volt, hogy a Hopfield hálózatok gyengeségeit kiküszöbölje. A \eqref{probFirst} képletet felhasználva megalkothatjuk a hibafüggvényünket, amely a negatív logaritmikus valószínûségi függvény (negative log likelihood function) lesz:
\begin{align}
\mathcal{L}(\theta, x) = \frac{1}{N}\sum_{x^{(i)} \in \mathcal{D}}log(p(x^{(i)})
\end{align}

Definiáljuk a szintén a temodinamikából származó szabad energia függvényt:
\begin{align}
\mathcal{F} = -log\sum_{h}e^{-E(x,h)}
\end{align}

Ezzel újradefiniálhatjuk a valószínûségi függvényt:
\begin{align}
p(x) = \frac{e^{-\mathcal{F}(x)}}{Z} \quad \textrm{ahol} \quad Z = \sum_{x}e^{-\mathcal{F}(x)}
\end{align}

Ami megengedi hogy a következõt írhassuk fel:
\begin{align} \label{eq:rbmPFromEnergy}
-\frac{\partial{logp(x)}}{\partial\theta} 
\approx 
\frac{\partial\mathcal{F}(x)}{\partial\theta} - 
\frac{1}{|\mathcal{N}|}
\sum_{\widehat{x} \in \mathcal{N}}
\frac{\partial \widehat{x}}{\partial \theta}
\end{align}

Ha az elõbbi függvényt deriváljuk, akkor megkapjuk a súlyváltozók update függvényét:
\begin{align} \label{eq:RbmWDirect}
-\frac{\partial logp(v)}{\partial W_{ij}} = E_v[p(h_i|v) * v_j] - v_j^{(i)} * sigm(W_i * v^{(i)} + c_i)
\end{align}
\begin{align} \label{eq:RbmCDirect}
-\frac{\partial logp(v)}{\partial c_i} = E_v[p(h_i|v)] - sigm(W_i * v^{(i)})
\end{align}
\begin{align} \label{eq:RbmBDirect}
-\frac{\partial logp(v)}{\partial b_j} = E_v[p(h_i|v) * v_j] - v_j^{(i)}
\end{align}

Ebbõl látható hogy az egyes deriváltak kiszámításához szükségünk van a rejtett neuronok valószínûségére a bemeneti neuronok állapotától függõen. Ezért szükséges hogy a boltzman gép korlátozott legyen, tehát egy páros gráp formáját vegye fel, mert így az egyes neuronokhoz tartozó valószínûségek párhuzamosítva számolhatóak, mivel függetlenek egymástól. Erre egy gyors eljárás a kontrasztív divergencia algoritmus amelynek a valózsínûségi mintavételét a \figref{gibbs}~ábra szemlélteti. A gibbs mintavételezéshez tartozó markov lánc lépéseinek a képleteit a \eqref{h1} és a \eqref{v1}~képlet írja le.

\begin{align} \label{eq:h1}
h^{n+1} \quad \sim \quad sigm(W^Tv{(n)} + c) 
\end{align}  

\begin{align} \label{eq:v1}
v^{n+1} \quad \sim \quad sigm(W^Th^{(n)} + b) 
\end{align}


\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/gibbs}
\caption{A gibbs mintavételezési eljárás. Forrás: \protect\url{http://recognize-speech.com/images/nicolas/Gibbs.png}} 
\label{fig:gibbs}
\end{figure}

Ahol $c$ és $b$ az eltolás súlyvektorokat jelzik. Ezekre a képletekre fogok majd a dolgozat késõbbi részeiben hivatkozni, amikor az általam megírt RBM struktúrát optimalizálom.

\paragraph{A DBN\cite{salakhutdinov2009deep}} A DBN (Deep Boltzmann Machine - Mély Boltzmann Gép) a legegyszerûbb formájában egymásra helyezett RBM rétegek sorozata, elõnye hogy a generatív modell képes hierarchikus jellemzõkiemelésre.

\paragraph{Az RBM és DBN elõtanulás} Az elõbbiekben bemutatott tanítással megcsinálhatjuk azt, hogy egy MLP egyes rétegeit rétegenként elõtanítjuk. Így a felügyelt tanítás elején az MLP súlyai már rögtön a bemenet tulajdonságaira lesznek rászabva.

\paragraph{Az RBM és DBN elõtanulás eredményei} A hálók elõtanításával jelentõs teljesítmény javulást értek el a kutatók nagy paraméter térrel rendelkezõ hálózatok esetében. Ez azt bizonyítja hogy valóban a jellemzõ reprezentációihoz közel helyezkedik el egy nagyon optimális lokális minimum. Az \tabref{MlpEloTablazat}~táblázat szemlélteti az eredményeit az MNIST adathalmazon. Látszik hogy ennek segítségével jóval nagyobb hálók képezhetõek ki és lényeges jobb eredményre vezetnek.

\begin{table}[ht]
	\footnotesize
	\centering
	\caption{Az MLP teljesítménye az MNIST adathalmazon RBM elõtanulást használva. Forrás: \cite{mnist}}
	\label{tab:MlpEloTablazat}
	\begin{tabular}{ | l | c | c |}
	\hline
	Rétegek száma & neuron struktúra & Teszt szet hiba százalék \\ \hline
	2-réteg & 800-10 & 0.7 \\
	5-réteg & 2500-2000-1500-1000-500-10 & 0.35 \\
	\hline
	\end{tabular}
\end{table}

\paragraph{További módszerek az RBM használatára osztályozáshoz} Az elõtanulásban nem merül ki az RBM jellegû hálózatok potenciálja, ha osztályozási feldatokról van szó, egy pár további lehetõséget szeretnék még érintõlegesen megemlíteni. 
\begin{itemize}
\item Egy másik, lehetõség a szerint az egyes osztályoknak egy RBM-et hozunk létre, és egy Softmax modell-t tanítunk a szabad energia függvényükön. A particiós függvényt ebben az esetben a softmax paraméterei approximálják.\cite{rbmGuide}
\item Egy harmadik lehetõség hogy két külön látható réteg csoportot tartunk, egyet a kép adatoknak, egy másikat pedig a tanító cimkéknek, ebben az esetben az osztályhoz tartozási valószínûséget a teszt vektor, és a cimkék szabad energiájájának eloszlásában határozzunk meg. Tehát minnél alacsonyabb lesz a szabad energia egy cimkével, annál valószínûbb hogy a teszt vektor abba az osztályba tartozik, itt végülis tanulásnál a cimkék és a mintapontok együttes valószínûségi sûrûségfüggvényét becsüljük.\cite{rbmGuide}
\item Egy újabb fejlemény Hugo Larochelle Hibrid RBM \cite{hrbm} struktúrája, amit közvetlen az RBM-en belül kombinálja a generatív és a diszkriminatív modellek elõnyeit, és egyszerre tud tanulni on-line jelleggel cimkézett és cimkézetlen adatokból egyaránt.
A struktúrát a \figref{hrbm}~ábra szemlélteti.
\item A Larochelle féle strúktúra továbbfejlesztését szintén a \figref{hrbm}~ábra szemlélteti. Ez az úgynevezett "Stacked Boltzmann Experts Network"\cite{sben}. Itt minden szint ad egy valószínûséget a tanító minta osztályzására, és ezeknek az átlagolásával születik meg a végleges predikció. Ezek a hálózatok azért végtelenül izgalmasak, mert habár ha sok a tanító minta, akkor ugyanolyan a teljesítménye mint a klasszikus módszereknek, de ha kevés cimkézet adat áll rendelkezésre, akkor vastagon jobb teljesítményt hoz, mitn ahogyan az a \figref{sbenPerf}~ábra is mutatja. Érdemes megemlíteni hogy a publikációban újságcikkeken, és nem képeken tesztelték.
\end{itemize}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/hrbm}
\caption{A (a) Larochelle féle hibrid RBM \cite{hrbm} és (b) annak a többszintû továbbfejlesztése az SBEN\cite{sben} struktúra.} 
\label{fig:hrbm}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=150mm, keepaspectratio]{figures/sbenPerf}
\caption{Az SBEN\cite{sben} és más, klasszikus struktúrák eredményei a WEBKB adathalmazon, úgy hogy csak a tanító adatok 1\%-a volt felcimkézve (8 példa osztályonként). A mérés teljes riportját az SBEN publikációban\cite{sben} lehet megtalálni.} 
\label{fig:sbenPerf}
\end{figure}

\subsection{State of the art MLP hálózatok}
\paragraph{További kutatások} Az MLP hálózatokat a mai napig nagy érdeklõdés övezi egyszerû struktúrájuk miatt. Látszik hogy az MNIST adathalmazon már nem nagyon van hova javítani az MLP-k teljesítményét, viszont mint azt pár paragrafussal elõbb megemlítettem a paraméter tér csökkentésében, és komplexebb adathalmazokhoz még van fejleszteni való ezeken a struktúrákon. Egy igen friss publikáció amelynek címe "How far can we go without convolution: Improving fully-connected networks"\cite{mlpFrontier} arra mutat rá hogy hogyan lehet az MLP hálózatok paraméter terét úgy csökkenteni hogy a sigmoid rétegek közé kis méretû lineáris rétegeket teszünk be, példának okáért legyen a két réteg 1500-2000 neuron, akkor a teljes paraméter terük $1500 * 2000 = 3 000 000$, de ha közé teszünk egy 500 neuronos lineáris rétege, akkor ez lecsökken $150 * 500 + 2000 * 500 = 1 075 000$ paraméterre, ami igen szignifikáns redukciót jelent a hálózat komplexításában. Ez a redukció oly mértékû hogy a fentebb említett publikációban vizsgált legnagyobb struktúra paraméter terét 112 millióról 2.5 millióra csökkentették, összehasonlítás képpen egy modern konvoluciós hálónak 3.5 millió paramétere van. Látható hogy ezzel sikerült a kutatóknak megoldania a többrétegû perceptron gépek egyik legnagyobb problémáját, a mértéktelenûl burjánzó paraméter teret. Ezen felül az eltünõ gradiensek problémáját is orvosolja, amivel itt bõvebben nem foglalkozunk. Viszont az eredményeit a CIFAR-10 \figref{mlpEvolution}~ábra szemlélteti. Az elõbb említett táblázat nagyon jól összefoglalja az MLP-k teljesítményének a fejlõdését a CIFAR-10es adathalmazt használva.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/mlpEvolution}
\caption{Az MLP fejlõdése a CIFAR-10 adahalmazon. (1) Logiszikus regresszió fehérített adatokon; (2) Tiszta backpropagation egy 782-10000-10 méretû hálózaton; (3)  Tiszta backpropagation egy 782-10000-10000-10 méretû hálózaton. (4) Egy 10000-10000-10es hálózaton, RBM elõtanítással, az utolsó réteg logisztikus regresszió; (5) Egyrétegû 10000 neuronos hálózat logisztikus regressziós kimenettel, RBM elõtanítással; (6) "Fastfood FFT" model (7) Zerobias autoencoder hálózat 4000 rejtett neuronnal és logisztikus regressziós kimenettel; (8) 782-4000-1000-4000-10 Z-Lin hálózat; (9) 782-4000-1000-4000-1000-4000-1000-4000-10 Z-Lin hálózat dropoutokkal; (10) Ugyanaz mint a (8), csak adat augmentációval. Az (1)-(5) eredmények Krizhevsky és Hinton 2009-es publikációjából származnak. A legutolsó azért szürkített, mert adat augmentációt használ. Forrás: \cite{mlpFrontier}} 
\label{fig:mlpEvolution}
\end{figure}

\paragraph{konkluzió} Ezzel a végére értem az MLP-vel való képosztályozás lehetõségeinek. Lászik hogy igen nagy eredmény javulást hoztak az új kutatások. A paraméter teret 112 milliórol 2.5 millióra csökkentették, és a CIFAR-10en a Hinton féle eredeti hálózathoz képest \emph{37\%}-ot javítottak a háló osztályozó képességén. De ez még mindig kevés a következõleg bemutatott struktúrákhoz képest.

\subsection{Konvoluciós hálózatok}
\paragraph{Bevezetés} A szakdolgozatom harmadik nagy részét a konvoluciós hálózatok \cite{leNet} teszik ki. Miután az irodalomkutatásom közben rá kellett jönnöm hogy az általam tanult klasszikus MLP struktúrák nem képesek a komplex jelek, mint például az MNIST adathalmaznál összetettebb képek kielégítõ megtanulására, kénytelen voltam új irányok után nézni. Így találtam meg a konvoluciós architektúrákat. Ezek korunk legjobban teljesítõ architektúrái, ennek oka hogy a jelek nagy része amit fel szeretnénk dolgozni az emberi érzékszervek által érzékelt jelek - példáúl képek - amik jelentõs transzlációs invarianciával rendelkeznek, és ezek a hálózatok ezt az a priori tudást beleépítik az architektúrába, így lényegesen kevesebb paramétert kell megtanulnunk mint egy \emph{elméleti síkon} hasonló teljesítményû MLP-nél. Természetesen tudjuk Cybenko (1989)\cite{cybenko} és Kurt Hornik (1991)\cite{hornik} publikációi után hogy egy kétrétegû MLP-vel tetszõleges függvényt képes approximálni, de ehhez annyi neuron kellene a komplex jelek esetében mint a képek hogy praktikusan nem kivitelezhetõek ezek a hálózatok. A rétegek növelésével és hálózati kényszerek bevezetésével ez a paraméter tér jelentõsen csökkenthetõ.

\paragraph{Az intuició} A konvoluciós architektúra teljes mértékben biológiailag inspirált, a macska vizuális kortexének a feltérképezésénél találtak hasonló kapcsolatokat az állat agyában\cite{cat}, és ennek a mintájára építették fel a mesterséges hálózatot. Alapötlete hogy magába a hálózati struktúrába foglaljuk bele a jel transzlációs invarianciáját. A modellt eleinte képek feldolgozására alkották meg, és az elõbbi mondat itt is szemléltethetõ a legintuitivabban. Tegyük fel hogy van egy képünk amin vagy egy objektum, akkor ha fel kell ismerni hogy a kép az adott objektumnak a jellemzõit tartalmazza-e akkor nekünk adott esetben ugyan annyi információval szolgál ha ez a jellemzõ (mondjuk egy sarok) a bal vagy a jobb oldalon van a képen. Természetesen ezek a lokális struktúrák a rétegekkel felfele egyre globálisabbak lesznek, az egyre magasabb szinteken pedig több jellemzõbõl komponált össztetett jellemzõk jelennek meg. És a hálózat az összetett jellemzõk jelenlétébõl következtet a kép osztályára. Ezt hivatott szemléltetni a \figref{deconvnet}~ábra és \figref{deconvnet2}~ábra amely egy konvoluciós háló egyes rétegeinek a szûrõit mutatja be, és hogy milyen képelemek aktiválták õket a leginkább. Az elsõ sikeres alkalmazása ennek a modellnek a LeNet\cite{leNet} volt 1990-ben, amelyet irányítószámok, karakterek és hasonló dolgok felismerésére használtak, de a modell sokáig nem kapott nagy érdeklõdést.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/deconvnet}
\caption{Az elsõ két szint szûrõi egy konvoluciós hálozatban. Forrás: "Visualizing and Understanding Convolutional Neural Networks" \cite{deconvnet}} 
\label{fig:deconvnet}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/deconvnet2}
\caption{Az felsõbb rétegek szûrõi egy konvoluciós hálozatban. Forrás: "Visualizing and Understanding Convolutional Neural Networks" \cite{deconvnet}} 
\label{fig:deconvnet2}
\end{figure}

\paragraph{Matematikai interpretációja} Tegyük fel hogy van egy $32x32x3$-as képünk. Ahhoz hogy egy teljes rétegbe kapcsoljuk bele mondjuk $1024$ neuronnal ki kellene lapítanunk, és egy $32 * 32 * 3 * 1024 = 3'145'728$ paraméterünk lenne az elsõ rétegben. De tegyük fel hogy a legkissebb jellemzõ amit érzékelni akarunk az egy $3x3$ méretû patchen van a képen, viszont akárhol lehet, akkor ha az elsõ rétegben $32$ jellemzõt szeretnénk érzékelni, akkor csak $3 * 3 * 32 = 288$ paraméterre lesz szükségünk az elsõ rétegben, ami jelentõs redukció. Ezután a következõ réteg ehhez a $288$ neuronhoz fog kapcsolódni, és ha ott 64 neuron lesz akkor $3*3*64 = 567$ neuronra kell majd, amik az eredeti képbõl viszont már egy $5x5$-ös szeletet fognak indirekt módon lefedni. Ezt mutatja be egy klasszikus architektúra a leNet a \figref{leNet}~ábrán.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/leNet}
\caption{Az felsõbb rétegek szûrõi egy konvoluciós hálozatban. Forrás: "Visualizing and Understanding Convolutional Neural Networks" \cite{deconvnet}} 
\label{fig:leNet}
\end{figure}
\paragraph{Az IMAGNET} 2012-ben az AlexNet nevû konvoluciós hálózat amelyet Alex Krizhevsky, Ilya Sutskever és Geoffrey Hinton alkottak fölényesen megnyerte az azévi ILSVRC versenyt. A háló top 5 hibája (az olvasó konzultáljon az adathalmazokat bemutató résszel a metrika leírásáért.) 16\% volt, míg a második helyete ami egy SVM-eket használó modell volt 26\%-os hibát produkált. Ez a 10\%-os különség az egekbe emelte a konvoluciós hálózatok népszerûségét, és hivatalosan is elhozta a neurális képfeldolgozás korát. Innentõl kezdve minden évben konvoluciós hálózatok nyerték meg az ILSVRC-t. A \tabref{ImagenetTable}~táblázat bemutatja az egyes évek eredményeit a hálót néhány paraméterével együtt. Összehasonlítás képpen, egy átlagos ember teljesítménye az adathalmazon 5-10 hibaszázalék körül mozog. 

\begin{table}[ht]
	\footnotesize
	\centering
	\caption{Az ILSVRC gyõztesei} \label{tab:ImagenetTable}
	\begin{tabular}{ | c | c | p{4cm} | c | c |}
	\hline
	Év & A struktúra neve & Tanítás ideje & Paraméter tér mérete & Top 5 hiba százalék \\ \hline
	2012 & AlexNet \cite{alexNet} & két GTX 580 GPU-n 5-6 nap & 60 millió & 16\% \\
	2013 & ZF Net \cite{deconvnet} & egy GTX 580 GPU-n 12 nap & ~60 millió & 11.2\% \\
	2014 & GoogLeNet \cite{googLeNet} & "néhány high end GPU-n egy héten belül" & 4 millió & 6.7\% \\ 
	2015 & Microsoft ResNet \cite{resNet} & 8 GPU-s gépen 2-3 hétig & N\textbackslash A & 3.6\% \\
	\hline
	\end{tabular}
\end{table}

\paragraph{Értékelés} Látható hogy a legújabb konvoluciós architektúrák már az emberi kiértékelésnél is pontosabb eredményt hoznak. Ráadásul a kezdeti naív megközelítést követõen a paraméter tér is drasztikus csökkenésnek indult. Manapség ha valaki képosztályozási feladatot szeretne végezni neurális hálózatokkal, akkor egy ilyen elõre elkészített architektúrát fog használni. Sajnos az is jól nyomonkövethetõ hogy a hálózatok fejlõdésével a szükséges hardware kapacitás is meredek emelkedésnek indult. Ha az ember egy konvoluciós architektúrát egy saját adathalmazra szeretne megtanítani akkor komoly infrastruktúrával kell rendelkeznie hozzá. Éppen itt domborodik ki a tensorflownak a dolgozat elején említett elõnye, hogy miután pythonban specifikáltuk a struktúrát azt képesek vagyunk minden erõfeszítés nélkül egy 8 GPU-s fürtre szétterjeszteni és tanítani, majd utána a paraméter teret lementve akár egy mobilon a megtanított hálót újra betölteni és akár valós idejû inferenciát futtatni. A paraméter tér ha 16 bites floatokkal számol az ember akkor 4 millió paraméternél kb 8 megabyte lesz, ami még egy igen kezelhetõ mennyiség.

\section{További érdekes irányok a neurális képfeldolgozásban}

Itt, az irodalom kutatásom tárgyalásának végén elértünk arra a pontra ahol a neurális képfeldolgozás már bevett módszereinek a nagyobb állomásait áttekintettük. Ebben a fejezetben röviden fel szeretném villantani a neurális képfeldolgozás legújabb és legérdekesebb irányait. 

\paragraph{Régió alapú konvoluciós hálózatok\cite{rcnn}} Sokan azt mondják hogy ez a publikáció csokor (R-CNN, Fast R-CNN, Faster R-CNN) hosszú idõk óta az egyik legfontosabb amit új neurális architektúrákról olvashatott az ember. Eddig meg tudtuk mondani egy hálóval hogy tartalmaz-e a kép valamilyen objektumot. Az R-CNN hálózatok már az objektum pontos helyét is megmondják a képen, ami egy minõségbeli ugrást jelent. Az \figref{FasterRCNN}~ábra szemlélteti a módszert. A módszer lényege hogy a feladat két neurális hálózatra van faktorizálva amik tandemben dolgoznak, az egyik egy osztály agnosztikus objektum detektor, míg a másik egy osztályozó hálózat.

\begin{figure}[!ht]
\centering
\includegraphics[width=70mm, keepaspectratio]{figures/FasterRCNN}
\caption{A Faster R-CNN munkafolyamata} 
\label{fig:FasterRCNN}
\end{figure}

\paragraph{Generatív adverziális hálózatok\cite{genAdvNets}} A LeCunn, a konvoluciós hálók megalkotója szerint ez az utóbbi 10 év legérdekesebb ötlete a területen. A lényeg hogy két hálózatot tanítunk tandemben, egy generatív és egy diszkriminatív modell-t. A diszkriminatív modell dolga edönteni egy képrõl hogy valódi-e vagy sem, a generatívé pedig hogy olyan képeket tudjon generálni amivel átveri a másik modell-t, ezért hívják adverziális hálozatnak. Az egész súlya abban rejlik hogy így a diszkriminatív hálózatnak meg kell tanulni az adat egy nagyon jó reprezentációját hogy képes legyen dönteni, ezzel mintegy nem felügyelt módon a legfontosabb jellemzõket kiemelni a képbõl. A generátor a végére pedig képes lesz valósághû képeket "álmodni". A \figref{adversarial}-ábra mutat egy tipikus tanító példát.

\begin{figure}[!ht]
\centering
\includegraphics[width=70mm, keepaspectratio]{figures/adversarial}
\caption{Jobbra: Eredeti kép, középen: Pertubációk, balra: Pertubált kép. A jobb oldalit helyesen, a bal-t pedig hibásan osztályozná egy CNN.} 
\label{fig:adversarial}
\end{figure}

\paragraph{Képleírások generálása\cite{genImgDesc}} A nem olyan távoli múltban nagyon sok érdekes publikáció jelent meg olyan neurális struktúrákról amelyek képek leírására alkalmasak. Lényegében egy CNN és egy RNN hálózat mûködik együtt, és fantasztikus dolgokra képesek. Tovább nem is taglalnám, mert nagyon sok elõismeretet igényel a téma, az érdeklõdõk a megfelelõ publikációt megtalálják az irodalomjegyzékben. A \figref{generatingImageDesc} mutatja az eredményt.

\begin{figure}[!ht]
\centering
\includegraphics[width=70mm, keepaspectratio]{figures/caption}
\caption{A Faster R-CNN munkafolyamata} 
\label{fig:generatingImageDesc}
\end{figure}

\paragraph{Térbeli transzformációs hálózatok (STN \cite{stn})} A fejlesztés jelentõsége abban rejlik hogy eddig mindig vagy a háló struktúrájába kellett belekódolni ha valamilyen variancia ellen védeni szerettük volna a hálózatot, vagy pedig az adathalmazt kellett úgy augmentálni hogy a háló jól általánosítson. Az elõbbire egy jó példa a CNN hálózatok max-pooling rétege, az utóbbira pedig az hogy mondjuk minden képet elforgatva is beadunk a háló tanításakor, hogy invariáns legyen rotációra a megtanult modell. Az STN hálózatok mint egy modulként kapcsolhatóak az egyes hálózatok elé, és ezeket a problémákat megfelelõ tanítás után automatikusan megoldják. Jól látható hogy a neurális fejlesztés a monolitikus hálózatokból szintén elkezdett a modulokból felépülõ paradigma felé menni. Az \figref{STN}~ábra mutat a hálózat mûködésére egy példát.

\begin{figure}[!ht]
\centering
\includegraphics[width=70mm, keepaspectratio]{figures/stn}
\caption{Egy térbeli transzformációs hálózat lépései.} 
\label{fig:STN}
\end{figure}

\paragraph{Irodalomkutatás összefoglalása} Ezzel az irodalom kutatásom végére értem. Úgy vélem hogy a neurális képosztályozás legtöbb fontos architektúrájára kitértem, és kaptam egy átfogó képet arról hogy hol tart a terület, és milyen módszerekkel érdemes nekiállni egy ilyen problémának. A továbbiakban szeretném bemutatni a saját munkámat, hogy mely hálózatokat implementáltam, mértem le a saját rendszeremen, és ebbõl milyen tanulságokat vontam le.

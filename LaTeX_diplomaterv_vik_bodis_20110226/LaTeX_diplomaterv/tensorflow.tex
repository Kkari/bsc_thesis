%----------------------------------------------------------------------------
\chapter{A Tensorflow\cite{tensorflow} ökoszisztéma áttekintése}

\section{A Tensorflow könyvtár.}

\paragraph{A technológiai választás indoklása} A modern szoftvermérnöki munka szerves része a rendelkezésre álló eszközök garmadájából a legmegfelelõbb kiválasztása. Ez a kínálat mérétéke, az információk elszórtsága és ellentmondásossága miatt koránt sem egy egyszerû feladat. A címbõl talán úgy sejlik hogy a tensorflow a munkámhoz már egy elõre eldöntött választás volt, de ez a megállapítás koránt sem lenne helytálló. A szakdolgozatom nulladik lépseként számos más - a neurális hálózatok fejlesztését támogató - könyvtárat vettem szemügyre. A teljesség igénye nélkül: Caffee, Chainer, CNTK, Matlab, Tensorflow, Thenao, Torch. A következõkben szeretném bemutatni az általam választott eszköz a Tensorflow felépítését, és ezzel mintegy implicit módon megindokolni hogy szerintem miért ez a megfelelõ eszköz a neurális hálózatokkal való munkához.

\paragraph{Bevezetõ} Munkám során a Tensorflow nevû könyvtárral dolgoztam. A Tensorflow egy elosztott, számítási gráf alapú numerikus könyvtár. A Goolge Deep Mind kutatócsoport szakemberei hozták létre azzal a céllal hogy saját modelljeiket fejlesszék és értékeljék ki benne. A könyvtár a DistBelief nevû keretrendszer egyenesági leszármazottjának tekinthetõ. A DistBelief csendben meghúzódva, de ott dolgozik a fejlett világ majdnem minden emberének az élete mögött, lévén hogy az Alphabet cégcsoport (A Google feldarabolásának utódvállalait összefogó holding) több mint 50 csapata adaptálta, és vértezte fel általa intelligenciával alkalmazását. Pár ismertebbet kiemelve: Google Search, Adwords, Google Maps, SteetView, Youtube, és természetesen a Google Translate. Miután évek tapasztalata gyülemlet fel az elsõ generációs könyvtáruk használata során, úgy érezték  itt az idõ hogy - szakítva a technológiai teherrel amit az elsõ generáció hibái miatt magukkal hordoztak - létrehozzák a következõ generációs gépi tanulás rendszerüket, ez lett a Tensorflow aminek a fõ célja skálázható, elosztott gépi tanulási algoritmusok (fõképp neurális hálózatok) fejlesztése.

\paragraph{A Tensorflow alapgondolata.} Mint már említettem a Tensorflow könyvtárban az ember a modelljeit egy adatfolyam-szerû számítási gráfként definiálhatja. Ennek a megközelítésnek az a nagy elõnye hogy nagyon jó skálázódási tulajdonságokkal rendelkezik. Miután a gráf csomópontjai az egyes számítások, ezek adott esetben hatékony módon szétoszthatók különbözõ eszközök, vagy akár egész gépek között szerverparkokban. A könyvtár ezen tulajdonságának még egy hosszabb részt fogunk késõbb szentelni.
A létrejött gráfra mint egy alaprajzra érdemes gondolni, amit aztán az egyes munkamenetetek (session-ök) példányosítanak. Ezek a munkamenetek inicializálják a változókat, és a munkamenet képes a gráf egyes csomópontjait lefuttatni, amik igény vezérelt módon minden a bemenetükre kapcsolódó csúcsot lefuttatnak amíg el nem érnek egy bemenetig vagy egy kívülrõl betáplált változóig. Miután a csomópont sikeresen lefutott a kimenetét a csatolt nyelv egy változójaként adja vissza, például egy Python vagy C++ tömbként. Fontos megjegyezni hogy futás közben a gráffal nem lehet érintkezni, az egy atomi egységként fut le, hogy minnél jobban ki lehessen optimalizálni a számításokat.

\section{A Tensorflow modellek monitorozása és hibamentesítése}

\paragraph{A Tensorboard} Mivel a modern gépi tanulás modellek hihetetlen összetettséggel bírnak, és nagyon sok mozgó alkatrészük van, ezért természetesen felmerül az igény hogy egy ilyen újszerû keretrendszerben ipari erõsségû monitorozó és hibakeresõ funkciók kerüljenek bele. Ezeket az elvárásokat a Tensorflow esetében a mellékelt Tensorboard alrendszer teljesíti, amelyet munkám során én is extenzíven használtam. Ebbõl kifolyólag most nem is bocsájtanám bõvebb tárgyalásra, hanem majd az önálló munka szekcióban ismertetném.

\section{A Tensorflow futás közben} 

\paragraph{A Tensorflow serving kiszolgáló rendszer} Miután a kutatólaborokból kikerültek az új gépi tanulás modellek nem elég õket csak publikálni, a cégek komoly hasznot remélnek tõlük. A Google is kijelentette hogy "Information Retrieval first company" helyett õk most már egy "AI first company". Ez természetesen egy hozzá illõ infrastruktúra nélkül elképzelhetetlen. Ezért is hozták létre a Tensorflowhoz a Tensorflow Servinget, ami egy flexibilis, magas rendelkezésre állású kiszolgálórendszer. A rendszer lehetõvé teszi új architektúrák kiprobálását, üzembe helyezését és A/B tesztelését, miközben egy stabil, verziózott API-t biztosít a kliensek számára. Természetesen ez mit sem érne ha nem skálázna gond nélkül hatalmas magasságokba, ezért egyszerûen integrálható a Kubernetes névre hallgató docker alapú cluster kezelõ rendszerrel.

\paragraph{A tensorflow skálázodása} Érdemes belegondolni hogy az adatközpont TCP (DCTCP) vagy az Infiniband kapcsolatok adott esetben akár több gigabyteos sebességet érhetnek el, ugyanakkor a mátrix szorzás - ami a gépi tanuló algoritmusoknak egy kardinális eleme - négyzetes ordót igényel. Tehát sokkal jobban megéri részgráfokat a csomópontok között szétosztani és inkább a hálózati többlettel kalkulálni, mint hogy egyetlen gépre bízzuk a feladatokat.
A másik végletben viszont egy másik elvárás helyezkedik el. Miután mondjuk egy osztályozási feladatnál a modellünket megtanítottuk felismerni valamit, tegyük fel hogy például egy, a látássérült embereknek készített alkalmazásban felismerni az forgalomjelzõ lámpákat és azok állapotát, természetes lenne az igény hogy ezt a rászoruló magával tudja vinni. A háló ugyan az, a súlyokat már megtanultuk, de most az egész modellünket egy mobil eszközön kell futtatni. Ez az eszköz az inferenciát jácint könnyedséggel bírná, csak a tanulást nem tudtuk volna kivitelezni rajta. Mérnökként logikus az igény hogy ehhez ne kelljen mégegyszer lefejleszteni a modell-t, így idõt és pénzt spórolva. Erre lehetõség van a keretrendszerrel.

\paragraph{Mobil környezet} A Tensorflow támogatja a mobil eszközön való futást, az elõbbi szcenárió a könyvtárral egy teljesen járható úttá válik. A modell-t asztali számítógépen fejlesztem, adatparkon tanítom, és egy mobil eszközön futtatom, mindezt jelentõsebb kód újraírás nélkül. Jó példája ennek a felhasználási módnak a Google Translate, legújabb, nagy felhajtást elérõ verziója. Ez az alkalmazás a nyelvi modelleket a Google irdatlan infrastruktúráján tanul folyamatosan, miközben az inferenciát a telefonkészüléken futtatják lokálisan. Itt igazándiból két neurális háló is szerepet jásztik, az egyik a szöveget ismeri fel a képen (feltehetõen egy faster R-CNN), a másik pedig az effektív fordítást végzi.

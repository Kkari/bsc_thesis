%----------------------------------------------------------------------------
\chapter{Hálózatok impelmentálása és elemzése tensorflowban}
%----------------------------------------------------------------------------

A következõ részben szeretném bemutatni saját mukámat és mérési eredményeimet. Az elõzõ részben bemutatott hálózatfajtákból megvalósítottam számtalan példányt tensorflowban, és méréseket végeztem rajtuk az MNIST és a CIFAR-10 adathalmazon. A munkám célja az volt hogy leteszteljem a tensorflow lehetõségeit kísérleti hálózatok kifejlesztésére és monitorozására. A fejezet a következõ részekre tagolható:
\begin{enumerate}
\item A baseline metódusok bemutatása.
\item A fejlesztési környezet bemutatása.
\item Az általam használt optimizátorok ismertetése.
\item Saját fejlesztések bemutatása.
\end{enumerate}

\section{A baseline osztályozók}
Természetesen nem lehet méréseket végezni referencia adatok nélkül, ezért a CIFAR-10 és az MNIST adathalmazon is lefuttattam két ismert, minden nehézség nélkül használható osztályzó algoritmust. Az egyik a Logisztikus regresszió volt, a másik pedig az SVM. Mindkettõ bemenetére közvetlen a kép pixeljeit tettem.

\section{A saját magam által kialakított fejlesztõkörnyezet}
A Tensorflow ökoszisztéma nagyon jó pontja a csúcsra járatott monitorozási lehetõségek, viszont nem triviális egy olyan struktúra kialakítása ahol az ember nagy hatékonysággal dolgozhat. Hosszas kísérletezés után a következõ munkafolyamatot találtam a legjobbnak:
\begin{itemize}
\item \emph{Gyors prototipizálás:} Erre a célra a Jupyter notebookokba írt TF-Slim magas szintû API-t találtam a legjobbnak, így nagyon gyorsan le lehet akármilyen ötletet tesztelni és kiértékelni.
\item \emph{Stabil modellek fejlesztése:} Ha egy modell túljutott a pár soros méreten, vagy tényleg egy nagyobb rendszer részeként szeretnénk használni akkor azt érdemesnek találtam osztályba foglalni, és a fejlesztéshez PyCharm IDE-t használni mert lényegesen gyorsabban lehet vele haladni komplex python kódnál mint a notebookokkal. 
\item \emph{A modellek kiértékelése:} A modellek kiértékelésére úgy gondolom hogy a Tensorflowval érkezõ Tensorboard a legalkalmasabb, mint majd látni fogja az olvasó a modelleim elemzésénél hogy ez az eszköz lehetõvé teszi komplex struktúrák elemzését egészen a tanulástól az igényelt rendszer erõforrásokig.
\item \emph{A modellbõl kinyert adatok részletes elemzése:} Erre a célra az klasszikus tudományos csomagok használatát találtam a legjobbnak jupyter notebookban alkalmazva, mert így egy helyen van a modell futtatása, a mérési eredmények és azt elemzõ kód.
\item \emph{Egzotikusabb modellek kivitelezése:} Ha az ember olyan modelleket szeretne kódolni amik túlnyúlnak a klasszikus struktúrákon, akkor kénytelen lenyúlni a Tensorflow eredeti programozási absztrakciójához, mint ahogyan azt az RBM esetében látni fogjuk. Ez az alacsony API hatalmas szabadságot ad a programozónak, de iszonyatosan bõszavú (verbose).
\end{itemize}

\section{A kísérletek alatt használt optimizátorok}
Nagyon sok optimizátor létezik a numerikus optimalizálási feladatok megoldására, ezt most nem is szeretném különösebben taglalni mert ez egy olyan terület amibõl már sok szép Phd. munka született és hozzávetõlegesen 20-30 oldallal növelné meg a dolgozatomat.
Dióhéjjban, én két optimizátorral kísérleteztem, a klasszikus \emph{sztochasztikus mini-batch} és az \emph{adaptiv momentum becslési} módszerrel. Az elsõ elõnye hogy kevesebbet kell számolni, de vagy konstans tanulási rátát használ, vagy kívülrõl kell valamilyen lehûléses algortimussal kontrollálni ezt, ami nehezebbé teszi a használatát. 
A második egy adaptív módszer, ahol az optimizátorba bele van építve a tanulási ráta beállítása, így dinamikusan tudja lekezelni a hibafelület topológiájának a változását. Ennek az algoritmusnak két hiperparamétere van, de ezeket nem feltétlen kell megadni, mert a módszer szerzõ mindkettõhöz megadtak általánosságban jól mûködõ értékeket.
A különözõ optimizátorok összehasonlításáról az érdeklõdõk \url{http://sebastianruder.com/optimizing-gradient-descent} ezen a honlapon olvashatnak egy nagyon jó összefoglalást.

\section{A saját implementációk bemutatása}
\subsection{A hálózatok monitorozása}
A Tensorflow érett API-t kínál a hálózat paramétereinek követésére tanulás közben, de nem ment le automatikusan minden egyes lefutáskor minden változó értékét, mivel ez egy modern hálózatnál több millió értéket is jelenthet. A VGG konvoluciós háló például 140 millió paramétert tartalmaz. A hálózat változóiról általánosságban a következõ értékeket mentettem le: minimum érték, maximum érték, közép érték és a standardizált szórásukat. hogy ezeket ne kelljen minden változóra kiadni, ezért a \listref{variableSummaries}-es függvényt alkalmaztam.

\begin{lstlisting}[frame=single,float=!ht,caption= A változók mentése, label=listing:variableSummaries, mathescape=true]
def variable_summaries(name, var):
    """Attach a lot of summaries to a Tensor."""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.scalar_summary('mean/' + name, mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
        tf.scalar_summary('sttdev/' + name, stddev)
        tf.scalar_summary('max/' + name, tf.reduce_max(var))
        tf.scalar_summary('min/' + name, tf.reduce_min(var))
        tf.histogram_summary(name, var)
\end{lstlisting}

\subsection{A logiszikus regresszió}
Az elsõ modell amit implementáltam Tensorflowban az a logisztikus regresszió volt. A célja az volt hogy összehasonlítsam a Tensorflow erõforrás igényét egy klasszikus python machine learning csomag, a Scikit-learn igényeivel. A struktúrát az <szám>~ábra szemlélteti. Egybõl látszódhat hogy az avatatlan szem számára a tensorboard eléggé nehezen értelmezhetõ lehet, ezért is szokták mondani hogy a tensorflownak a tanulási görbéje viszonylag nagy. Szerencse hogy lehet benne névtereket létrehozni hogy a gráfok könnyebben átláthatóak legyenek. Az egész tanulási gráfot a járulékos nodeokkal az <szám>~ábra szemlélteti.

\subsection{A többrétegû perceptron}
A többrétegû perceptronnal való kísérletezést a TF-Slim API nagyon megkönnyíti, minden nehézség nélkül a rétegeket egymás után tenni, ha pedig egyedi elemet szeretne az ember definiálni akkor arra is lehetõség van. Hasonló architektúrákat teszteltem az MNIST és a CIFAR-10 adathalmazon is, ami nagyon jól megfogta a két adathalmaz közötti különbséget. A \listref{MlpDef}-listázás egy ilyen háló definicióját szemlélteti.

\begin{lstlisting}[frame=single,float=!ht,caption= Egy MLP definiciója, label=listing:MlpDef, mathescape=true]
def fully_connected(batch_data, batch_labels):
    with slim.arg_scope([slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
        # First Layer
        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')
        variable_summaries('fc/fc_1', x)
        
        # Second Layer
        x = slim.fully_connected(x, 1024, scope='fc/fc_2')
        variable_summaries('fc/fc_2', x)
        
        # Third Layer
        last_layer = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_3')
        variable_summaries('fc/fc_3', x)
        predictions = tf.nn.softmax(x)
 
    	slim.losses.softmax_cross_entropy(last_layer, batch_labels)
	    total_loss = slim.losses.get_total_loss()
   	 	tf.scalar_summary('losses/total_loss', total_loss)
    
    	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
        return optimizer, predictions
\end{lstlisting}

Mint látható nagyon könnyen állíthatóak a reguralizációs tagok, megadható több félet optimalizáló és hibafüggvény is, ami igazán könnyûvé tette a kísérletezést.

Az MLP-ben használt elemi alkotóelemeim:
\begin{itemize}
\item \emph{fc:} A teljesen kapcsolt réteg, minden neuron, minden elõbbi réteg neuronjával össze van kötve, ennek a definicióját a \eqref{fullyConnected}-képlet mutatja be, ahol l a réteg sorszáma, és "Activation" egy tetszõleges aktivációs függvény.

\item \emph{reLu:} A rektifikált lineáris egység (reLu) egy aktivációs függvény, ezeket a teljesen kapcsolt réteg után kapcsoljuk, azért hogy nem-linearitásokat vigyünk a rendszerbe. így növelve a leképzõ képességét. A reLu definiciója a \eqref{reLu}~képleten látható.
\item \emph{sgm:} A sigmoid aktivációs függvény amely szintén egy aktivációs függvény mint a relu, de nehezebb a deriváltját számolni, és érzékenyebb az adatok normalizáltságára. Az sigmoid definiciója \eqref{sigmoid}~képleten látszik.
\end{itemize}

\begin{align} \label{eq:fullyConnected}
X^{(l)} = W*Activation(X^{(l-1)})
\end{align}

\begin{align} \label{eq:reLu}
reLu(x) = max(0, x)
\end{align}

\begin{align} \label{eq:sigmoid}
sgm(x) = \frac{1}{1 + e^x}
\end{align}


\subsection{Az RBM hálózat}
Mint mondtam a TF-Slim nagyon kényelmes volt addig amíg olyan struktúrákkal dolgoztam amik könnyen definiálhatóak. Viszont csak többrétegû perceptronok és konvoluciós hálózatokat lehet benne egyelõre alkotni. Az korlátozott boltzmann gép implementálásához le kellett mennem a Tensorflow alacsony szintû interfacéhez amiben a hálózat implementálása több mint egy hét volt. Viszont ezalatt értettem meg igazán hogy hogyan mûködik egyrészt a könyvtár, másrészt pedig az RBM-ek. 
Az RBM struktúra legtrükkösebb része a kontrasztív divergencia volt, mivel az egy valószínûségi döntés, de az egyes batchek futása közben nem tudok közvetlenül a gráfon belûl véletlen számokat generálni változtatható mennyiségben. Azért nem lehet egy adott méretben generálni, mert a batch méret változik, és minden számnak kellett generálni.
A megoldás amit alkalmaztam az az volt hogy numpy-ban legeneráltam minden tanításnál egy akkora mátrixot véletlen számokból mint amekkora a tanító halmazom volt. Majd a bináris 0-1 döntést a következõ képpen szimuláltam:
\begin{enumerate}
\item Kivontam a valósziníségi mátrixot a random számokból
\item Vettem az eljöjelét a keletkezett mátrixnak
\item Átvezettem egy relu rétegen, amitõl 0 és 1 közé normalizálódott.
\end{enumerate}

\paragraph{A tanítás} A tanítást kétféle képpen végeztem el, egyrészrõl definiáltam a szabad energia függvényt és a keretrendszerrel számoltattam ki a \eqref{rbmPFromEnergy} függvénybõl és különbözõ beépített optimalizátorokkal teszteltem, gradientdescenttel és adaptív gradiens (ADAM) optimizátorral. Másrészt közvetlen kiszámoltam a gibbs sampling utáni értékekbõl \eqref{RbmWDirect}, \eqref{RbmCDirect} és \eqref{RbmBDirect} függvényeket és egyszerûen csak hozzáadtam õket a súlyvektorhoz. A \figref{RbmClosed}~ábra a hálózat madártávlati struktúráját szemlélteti. Illetve a kísérletezés egy másik dimenziója az volt hogy egyes esetekben megengedtem az RBM felé kapcsolt regresszornak hogy az elõre megtanult súlyokat változtassa (ezt hívjuk fine-tuning-nak), más esetben pedig nem. Könnyen kivehetõ hogy egy softmax réteg is hozzá van csatolva az RBM magjához, miután felügyelet nélkül tanítom az RBM-et az a réteg végzi az osztályzást. A \figref{rbmExpanded}~ábra pedig az RBM belsõ strukturját mutatja. Itt látszik igazán hogy a Tensorboard grafikonjai milyen kifonomult vizualizációt tesznek lehetõvé. A tanítás optimalizálását a \cite{rbmGuide}-ben leírtak alapján végeztem, de az osztályozásban meg hoztak érzékelhetõ javulást, ezért ezeket a méréseknél nem fejtem ki részletesen.

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/rbmClosed}
\caption{} 
\label{fig:rbmClosed}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/rbmExpanded}
\caption{} 
\label{fig:rbmExpanded}
\end{figure}

\subsection{A DBN struktúra}
Miután az RBM struktrát megalkottam természetesen le szerettem volna tesztelni többrétegû elõre tanított hálózatok teljesítményét is. Erre nem a saját RBM hálózatomat használtam, hanem egy külsõ könyvtárat, ami viszont hibás volt úgyhogy helyenként meg kellett foltoznom. Itt jött jól az a tudás amit az RBM programozásánál szereztem, mert magabiztosan mozogtam a pythonban írt tensorflow kódban.

\subsection{Hibrid modellek}
A kísérletezésem során elkezdtünk olyan architektúrákkal foglalkozni ahol a kimenet nem a neurális hálózat része, mint például az RBM osztályozók általam tesztelt variánsánál, hanem a kimeneti aktivációk valószínûségi eloszlását adtam be mintánként egy SVM-nek. Így hatalmas dimenzió csökkenést értünk el, pl 784-rõl 64-re az MNIST esetében, ami jelentõsen meggyorsította az SVM tanítását, anélkül hogy a nyers pixeladatokon tanított SVM-hez képest romlott volna a modell predikciós képessége. Az általam használt teszt gépen a CIFAR-10es adathalmaz nyers pixeljeit iszonyatosan hosszú idõ alatt tudtam megtanítani egy SVM-nek. Ezért letömörítettem az az 3072 dimenziós CIFAR-10 adathalmazt egy 783 dimenziós térbe RBM-ek segítségével ami mindemellé még jellemzõ kiemelést is végzett, és ott tanítottam rajta a szupport vektor gépet, remélve hogy jó osztályozási eredményeket kapok. Ezt két megközelítésben próbáltam meg, az egyik az volt amikor egyetlen RBM-em volt, 3072 bemeneti és 783 kimeneti neuronnal, a másik az volt amikor 3 RBM-et tanítottam meg, minden színcsatornára egyet, és utána ezeket egy tömbbé összefûzve adtam át az SVM-nek. A két megközelítést a \figref{svmRbm}~ábra szemlélteti.

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/svmRbm}
\caption{} 
\label{fig:svmRbm}
\end{figure}

\subsection{A Konvoluciós modell}
Több konvoluciós modell-t kipróbáltam, a méréseim egyik fõ iránya az volt hogy ugyanazt a modellt probálom ki az MNIST és a CIFAR-10 adathalmazon, és megnézni hogy melyik modell hogyan reagál az adat megnövekedett komplexitására. Több hálózatot kiróbáltam, az alapmodell felépítését a \figref{convnet_1}~ábra mutatja. A kísérleteim arra irányultak hogy plusz teljesen összekötött rétegek hozzáadása, esetleg a konvoluciós rétegek más elrendezése milyen eredményre juttat.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/convnet_1}
\caption{} 
\label{fig:convnet_1}
\end{figure}

Az egyes operációkat szeretném megymagyarázni röviden amiket a konvoluciós modelljeimben használtam:
\begin{itemize}
\item \emph{conv(magasság, szélesség, mélység):} Kétdimenziós, diszkrét konvolució. Ahol a magasság és a szélesség adják meg a képfolt nagyságát. A mélység pedig hogy hány neuron legyen az adott rétegben. A kétdimenziós konvolicó definicióját a \eqref{convDef}-képlet mutatja be.
\item \emph{maxpool(magasság, szélesség)} Egy alulmintavételezési operátor a térbeli dimenziókban(magasság, szélesség). Ezzel csökkentjük drasztikusan a jellemzõterek nagyságát, és adunk transzlációs invarianciát a rendszerhez, mindig a legerõsebb jelet viszi keresztül a pooling foltból.
\item \emph{fc(neuronok száma):} A teljesen kapcsolt rétegeket a hálózat végére kapcsoljuk. Amíg a konvoluciós rétegek egy magas absztrakcióval rendelkezõ, viszonylag transzláció invariáns jellemzõteret nyújtanak a képrõl, addig a hálózat végén lévõ teljesen kapcsolt rétegek teszik lehetõvé a nemlineáris leképzéseket ebben a jellemzõtérben. Az itteni teljesen kapcsolt réteget és a hozzá tartozó nemlineritásokat is az \eqref{fullyConnected}, \eqref{sigmoid}, \eqref{reLu} írja le.
\item \emph{lrn:} Elvileg a reLu aktivációs függvény nem érzékeny annyira az adatok normalizálására, mégis kiderül a \cite{alexNet} publikációból hogy egy lokális normalizálási eljárás statisztikailag szignifikáns javulást képest hozni a hálózat osztályozó képességében. Ezt a matematikai struktúrát a \eqref{Lrn}~képlet szemlélteti. Ez is egy biológiailag inspirált eljárás. A bilógiai neve ennek az eljárásnak laterális inhibició.
\end{itemize}

\begin{align}
\label{eq:convDef}
h^k_{ij} = activation((W^k * x)_{ij} + b_{k}) \quad ahol \quad * \text{ konvoluciós operátor.}
\end{align}
\begin{align}
b^i_{x,y} = a^i_{x,y}/\Bigg(k + \alpha\sum^{min(N-1,i+n/2)}_{j=max(0,i-n/2)}(a^j_{x,y})^2\Bigg)^\beta \\
\text{Ahol } a^i_{x,y} \text{ az i-edik kernel-t alkalmazzuk az (x,y) beli pozicióra.} \notag \\ 
b^i_{x,y} \text{ a normalizáció utáni eredmény} \notag\\
\text{És n a vizsgált pozició szomszédos kernel térképei, N az összes} \notag\\
\text{kernel a rétegben és } \alpha,\beta \text{ és k pedig további hiperparaméterek.} \notag
\end{align}

\subsection{A konvoluciós modell skálázása}
\paragraph{Indoklás} Az elõbbiekben bemutatott konvoluciós modell, és amikkel általánosságban a szakdolgozat keretén belül kísérleteztem könnyedén taníthatóak pár perc/óra alatt egy korszerû CPU-n. Viszont ha az ember nagyobb modelleket szeretne tanítani akkor ez már nem egy reális alternatíva. Ezekre az esetekre egy nagyobb hálózattal kísérleteztem a CIFAR-10 adathalmazon amit az alábbi instrukciók alapján készítettem el\cite{scalingCNN}, aminek a felépítését a \figref{convnet_2}~ábra mutatja. 

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/convnet_2}
\caption{} 
\label{fig:convnet_2}
\end{figure}

\paragraph{Számítás elosztás}Ezt a hálózatot teszteltem CPU-n, 1 GPU-n, majd 2 GPU-n. A Tensorflow képes a számításokat automatikusan elosztani az eszközök között olyan módon hogy heurisztikusan megkeresi hogy mely operációk mentén érdemes szétvágni a számítási gráfot, majd azokhoz az élekhez berak küldõ és fogadó csomópontokat, ahogyan a \figref{device_distro}~ábra szemlélteti.
Ez sok esetben teljesen megfelel az elvárásainknak, ha nem szeretnénk sokat veszõdni a hálózat elosztásával, vagy amúgy is túl nagy a hálónk, és nem férne el gradiens számítással együtt rendesen egyetlen eszközön. De ha kissebb a háló és több eszközünk van akkor felmerülhet a lehetõség hogy esetleg jobban megérné a gráfot egy az egyben lemásolni az egyes eszközökre, és a súlyokat a fõ memóriában tartani, majd az egyes párhuzamos futások után itt szinkronizálni a lefutásokat. Én ezt a megközelítést teszteltem le, aminek a neve torony párhuzamos tanítás, ezt a \figref{tower_parallel}~ábra mutatja.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/device_distro}
\caption{} 
\label{fig:device_distro}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/tower_parallel}
\caption{} 
\label{fig:tower_parallel}
\end{figure}

\paragraph{Adatok dinamikus felolvasása} Természetesen az a kérdés is felmerülhet az emberben hogy ez nagyon jó hogy így el tudjuk osztani a számításokat, de az is probléma lehet ha a tanuló adatok nem férnek be a memóriába, akkor nem tudjuk rendesen tanítani õket, kivéve ha valamilyen bonyolult felolvasási kódot irunk hozzá. Hál istennek ezt nem kell megirnunk, hanem be van építve a keretrendszerbe, a Tensorflow erre direkt felolvasási pipelineokat állít a programozó rendelkezésére ennek a felépítését szemlélteti a \figref{tfPipeline}~ábra. Egy ilyen pipelinenak a következõ paraméterei vannak:
\begin{itemize}
\item Bemeneti file nevek.
\item A felolvasásra és elõfeldolgozásra használni kívánt szálak száma.
\item A megállási kritérium
\item A pipeline mérete, hogy hány még meg nem tanított kép legyen mindig a pipelineban.
\end{itemize}
Ezek után a keretrendszer gondoskodik arrol hogy a szálak mind stratifikáltan, külön fileokból olvassanak fel, illetve ha példányosítunk egy koordinátor objektumot és beregisztráljuk hozzá a pipeline-t akkor a szálak közötti hibakezelésrõl is gondoskodik, hogy egy szál hibája esetén ne akadjon ki a rendszer.
Ezen felül definiálhatunk még elõfeldolgozási lépéseket minden képhez, az általam használt implementációban például egyszerre 20 feldolgozatlan képet tart a memóriában, ezeket elõször közelítõleg fehéríti (ez is beépített funkció), majd ezt az adathalmazt négyszerezi a következõ képpen:
\begin{itemize}
\item A képek véletlenszerû tükrözése.
\item A kép világosságába való véletlenszerû zaj bevezetése.
\item A kép kontrasztjának véletlenszerû torzítása.
\end{itemize}
Ez jelentõsen növeli a háló általánosító képességét. Majd ezeket az adatokat beadja egy új sorba, ami véletlenszerû batcheket generál a 16 szál képeibõl, és megadhatjuk hogy hány modell vegye kis és dolgozza fel õket egyszerre. Esetemben a torony-parallel elrendezésben ez a GPU-k számától függ.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/tfPipeline}
\caption{} 
\label{fig:tfPipeline}
\end{figure}


%----------------------------------------------------------------------------
\chapter{Hálózatok impelmentálása és elemzése tensorflowban}
%----------------------------------------------------------------------------

A következõ részben szeretném bemutatni saját munkámat és mérési eredményeimet. Az elõzõ részben bemutatott hálózatfajtákból megvalósítottam néhány példányt Tensorflowban és méréseket végeztem rajtuk az MNIST és a CIFAR-10 adathalmazon. A munkám célja az volt hogy leteszteljem a Tensorflow lehetõségeit kísérleti hálózatok kifejlesztésére és monitorozására. A fejezet a következõ részekre tagolható:
\begin{enumerate}
\item A baseline metódusok bemutatása.
\item A fejlesztési környezet bemutatása.
\item Az általam használt optimizátorok ismertetése.
\item Saját fejlesztések bemutatása.
\end{enumerate}

\section{A baseline osztályozók}
Természetesen nem lehet méréseket végezni referencia adatok nélkül, ezért a CIFAR-10 és az MNIST adathalmazon is lefuttattam két ismert, minden nehézség nélkül használható osztályzó algoritmust. Az egyik a Logisztikus regresszió volt, a másik pedig az SVM. Mindkettõ bemenetére közvetlen a kép pixeljeit tettem.

\section{A saját magam által kialakított fejlesztõkörnyezet}
A Tensorflow ökoszisztéma nagyon jó pontja az érett monitorozási lehetõségek\cite{Tensorboard}, viszont nem triviális egy olyan struktúra kialakítása ahol az ember nagy hatékonysággal dolgozhat. Hosszas kísérletezés után a következõ munkafolyamatot találtam a legjobbnak:
\begin{itemize}
\item \emph{Gyors prototipizálás:} Erre a célra a Jupyter notebookokba írt TF-Slim magas szintû API-t találtam a legjobbnak, így nagyon gyorsan le lehet akármilyen ötletet tesztelni és kiértékelni.
\item \emph{Stabil modellek fejlesztése:} Ha egy modell túljutott a pár soros méreten, vagy tényleg egy nagyobb rendszer részeként szeretnénk használni akkor azt érdemesnek találtam osztályba foglalni. A fejlesztéshez PyCharm IDE-t használtam, mert lényegesen gyorsabban lehet vele haladni komplex python kódnál mint a notebookokkal. 
\item \emph{A modellek kiértékelése:} A modellek kiértékelésére úgy gondolom hogy a Tensorflow-val érkezõ Tensorboard a legalkalmasabb, mint majd látni fogja az olvasó a modelljeim elemzésénél hogy ez az eszköz lehetõvé teszi komplex struktúrák elemzését egészen a tanulástól az igényelt rendszer erõforrásokig.
\item \emph{A modellbõl kinyert adatok részletes elemzése:} Erre a célra az klasszikus tudományos csomagok használatát találtam a legjobbnak jupyter notebookban alkalmazva, mert így egy helyen van a modell futtatása, a mérési eredmények és azt elemzõ kód.
\item \emph{Egzotikusabb modellek kivitelezése:} Ha az ember olyan modelleket szeretne kódolni amik túlnyúlnak a klasszikus struktúrákon, akkor kénytelen lenyúlni a Tensorflow eredeti programozási absztrakciójához, mint ahogyan azt az RBM esetében látni fogjuk. Ez az alacsony API hatalmas szabadságot ad a programozónak, de iszonyatosan bõszavú (verbose).
\end{itemize}

\section{A kísérletek alatt használt optimizátorok}
Nagyon sok optimizátor létezik a numerikus optimalizálási feladatok megoldására, mivel a dolgozatom fókusza nem ezeknek a bemutatása, ezért csak futólag szeretném itt megemlíteni az általam használt változatokat.
Én két optimizátorral kísérleteztem, a klasszikus \emph{sztochasztikus mini-batch} és az \emph{adaptiv momentum becslési}\cite{kingma2014adam} módszerrel. Az elsõ elõnye hogy kevesebbet kell számolni, de vagy konstans tanulási rátát használ, vagy kívülrõl kell valamilyen lehûléses algoritmussal kontrollálni ezt, ami nehezebbé teszi a használatát. 
A második egy adaptív módszer, ahol az optimizátorba bele van építve a tanulási ráta beállítása, így dinamikusan tudja lekezelni a hibafelület topológiájának a változását. Az algoritmusnak két hiperparamétere van, melyre a szerzõk által megadott ajánlott értékek általában jó eredményre vezetnek.
A különbözõ optimizátorok összehasonlításáról az érdeklõdõk \url{http://sebastianruder.com/optimizing-gradient-descent} ezen a honlapon olvashatnak egy nagyon jó összefoglalást.

\section{A saját implementációk bemutatása}
\subsection{A hálózatok monitorozása}
A Tensorflow érett API-t kínál a hálózat paramétereinek követésére tanulás közben, de nem ment le automatikusan minden egyes lefutáskor minden változó értékét, mivel ez egy modern hálózatnál több millió értéket is jelenthet. A VGG konvolúciós háló például 140 millió paramétert tartalmaz. A hálózat változóiról általánosságban a következõ értékeket mentettem le: minimum érték, maximum érték, közép érték és a standardizált szórásukat. Annak érdekében hogy ezeket ne kelljen minden változóra kiadni, ezért a \listref{variableSummaries}-es függvényt alkalmaztam.

\begin{lstlisting}[frame=single,float=!ht,caption= A változók mentése, label=listing:variableSummaries, mathescape=true]
def variable_summaries(name, var):
    """Attach a lot of summaries to a Tensor."""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.scalar_summary('mean/' + name, mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
        tf.scalar_summary('sttdev/' + name, stddev)
        tf.scalar_summary('max/' + name, tf.reduce_max(var))
        tf.scalar_summary('min/' + name, tf.reduce_min(var))
        tf.histogram_summary(name, var)
\end{lstlisting}

\subsection{A logiszikus regresszió}
Az elsõ modell amit implementáltam Tensorflowban az a logisztikus regresszió neurális megközelítése volt. A célja az volt hogy összehasonlítsam a Tensorflow erõforrás igényét egy klasszikus python machine learning csomag, a Scikit-learn igényeivel.

\subsection{A többrétegû perceptron}
A többrétegû perceptronnal való kísérletezést a TF-Slim API nagyon megkönnyíti, minden nehézség nélkül a rétegeket egymás után tenni, ha pedig egyedi elemet szeretne az ember definiálni akkor arra is lehetõség van. Hasonló architektúrákat teszteltem az MNIST és a CIFAR-10 adathalmazon is, ami nagyon jól megfogta a két adathalmaz közötti különbséget. A \listref{MlpDef}-listázás egy ilyen háló definícióját szemlélteti.

\begin{lstlisting}[frame=single,float=!ht,caption= Egy MLP definíciója, label=listing:MlpDef, mathescape=true]
def fully_connected(batch_data, batch_labels):
    with slim.arg_scope([slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),
                      weights_regularizer=slim.l2_regularizer(0.0005)):
        # First Layer
        x = slim.fully_connected(batch_data, 400, scope='fc/fc_1')
        variable_summaries('fc/fc_1', x)
        
        # Second Layer
        x = slim.fully_connected(x, 1024, scope='fc/fc_2')
        variable_summaries('fc/fc_2', x)
        
        # Third Layer
        last_layer = slim.fully_connected(x, 10, activation_fn=None, scope='fc/fc_3')
        variable_summaries('fc/fc_3', x)
        predictions = tf.nn.softmax(x)
 
    	slim.losses.softmax_cross_entropy(last_layer, batch_labels)
	    total_loss = slim.losses.get_total_loss()
   	 	tf.scalar_summary('losses/total_loss', total_loss)
    
    	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
        return optimizer, predictions
\end{lstlisting}

Mint látható nagyon könnyen állíthatóak a reguralizációs tagok, megadható többféle optimalizáló és hibafüggvény is, ami igazán könnyûvé tette a kísérletezést.

Az MLP-ben használt elemi alkotóelemeim:
\begin{itemize}
\item \emph{fc:} A teljesen kapcsolt réteg, minden neuron, minden elõbbi réteg neuronjával össze van kötve, ennek a definícióját a \eqref{fullyConnected}-képlet mutatja be, ahol l a réteg sorszáma, és "Activation" egy tetszõleges aktivációs függvény.

\item \emph{reLu:} A rektifikált lineáris egység (reLu) egy aktivációs függvény, ezeket a teljesen kapcsolt réteg után kapcsoljuk, azért hogy nem-linearitásokat vigyünk a rendszerbe. így növelve a leképzõ képességét. A reLu definíciója a \eqref{reLu}~képleten látható.
\item \emph{sgm:} A sigmoid aktivációs függvény amely szintén egy aktivációs függvény mint a relu, de nehezebb a deriváltját számolni, és érzékenyebb az adatok normalizáltságára. Az sigmoid definíciója \eqref{sigmoid}~képleten látszik.
\end{itemize}

\begin{align} \label{eq:fullyConnected}
X^{(l)} = W*Activation(X^{(l-1)})
\end{align}

\begin{align} \label{eq:reLu}
reLu(x) = max(0, x)
\end{align}

\begin{align} \label{eq:sigmoid}
sgm(x) = \frac{1}{1 + e^x}
\end{align}


\subsection{Az RBM hálózat}
Mint mondtam a TF-Slim nagyon kényelmes volt addig amíg olyan struktúrákkal dolgoztam amik könnyen definiálhatóak. Viszont csak többrétegû perceptronok és konvolúciós hálózatokat lehet benne egyelõre alkotni. Az korlátozott boltzmann gép implementálásához le kellett mennem a Tensorflow alacsony szintû interfacéhez amiben a hálózat implementálása több mint egy hét volt. Viszont ezalatt értettem meg igazán hogy hogyan mûködik egyrészt a könyvtár, másrészt pedig az RBM-ek. 
Az RBM struktúra legtrükkösebb része a kontrasztív divergencia volt, mivel az egy valószínûségi döntés, de az egyes batchek futása közben nem tudok közvetlenül a gráfon belül véletlen számokat generálni változtatható mennyiségben. Azért nem lehet egy adott méretben generálni, mert a batch méret változik, és minden számnak kellett generálni.
A megoldás amit alkalmaztam az az volt hogy numpy-ban legeneráltam minden tanításnál egy akkora mátrixot véletlen számokból mint amekkora a batch mérete volt és ezt egy placeholderen keresztül injektáltam bele a hálóba. Majd a bináris 0-1 döntést a következõképpen szimuláltam:
\begin{enumerate}
\item Kivontam a valószínûségi mátrixot a véletlenszerûen generált számokból
\item Vettem az elõjelét a keletkezett mátrixnak
\item Átvezettem egy relu rétegen, aminek a kimenete az elõjeltõl függõen stabil 0 vagy 1 lett.
\end{enumerate}

\paragraph{A tanítás} A tanítást kétféle képpen végeztem el, egyrészrõl definiáltam a szabad energia függvényt és a keretrendszerrel számoltattam ki a \eqref{rbmPFromEnergy} függvénybõl és különbözõ beépített optimalizátorokkal teszteltem, sztochasztikus gradiens (SGD) és adaptív gradiens (ADAM) optimizátorral. Másrészt közvetlen kiszámoltam a Gibbs sampling utáni értékekbõl \eqref{RbmWDirect}, \eqref{RbmCDirect} és \eqref{RbmBDirect} függvényeket és egyszerûen csak hozzáadtam õket a súlyvektorhoz. A \figref{rbmClosed}~ábra a hálózat madártávlati struktúráját szemlélteti. Illetve a kísérletezés egy másik dimenziója az volt hogy egyes esetekben megengedtem az RBM felé kapcsolt regresszornak hogy az elõre megtanult súlyokat változtassa (ezt hívjuk fine-tuning-nak), más esetben pedig nem. Könnyen kivehetõ hogy egy softmax réteg is hozzá van csatolva az RBM magjához, miután felügyelet nélkül tanítom az RBM-et az a réteg végzi az osztályzást. A \figref{rbmExpanded}~ábra pedig az RBM belsõ struktúráját mutatja. Itt látszik igazán hogy a Tensorboard grafikonjai milyen kifinomult vizualizációt tesznek lehetõvé. A tanítás optimalizálását a \cite{rbmGuide}-ben leírtak alapján végeztem, de az osztályozásban meg hoztak érzékelhetõ javulást, ezért ezeket a méréseknél nem fejtem ki részletesen.

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/rbmClosed}
\caption{} 
\label{fig:rbmClosed}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/rbmExpanded}
\caption{} 
\label{fig:rbmExpanded}
\end{figure}

\subsection{A DBN struktúra}
Miután az RBM struktúrát megalkottam természetesen le szerettem volna tesztelni többrétegû elõre tanított hálózatok teljesítményét is. Erre nem a saját RBM hálózatomat használtam, hanem egy külsõ könyvtárat, ami viszont hibás volt úgyhogy helyenként meg kellett foltoznom. Itt jött jól az a tudás amit az RBM programozásánál szereztem, mert magabiztosan mozogtam a pythonban írt tensorflow kódban.

\subsection{Hibrid modellek}
A kísérletezésem során elkezdtem olyan architektúrákkal foglalkozni ahol a kimenet nem a neurális hálózat része, mint például az RBM osztályozók általam tesztelt variánsánál, hanem a kimeneti aktivációk valószínûségi eloszlását adtam be mintánként egy SVM-nek. Így jelentõs dimenzió csökkenést értünk el, pl 784-rõl 64-re az MNIST esetében, ami kifejezetten meggyorsította az SVM tanítását, anélkül hogy a nyers pixeladatokon tanított SVM-hez képest romlott volna a modell predikciós képessége. Az általam használt teszt gépen a CIFAR-10es adathalmaz nyers pixeljeit nagyon hosszú idõ alatt tudtam megtanítani egy SVM-nek. Ezért letömörítettem az az 3072 dimenziós CIFAR-10 adathalmazt egy 783 dimenziós térbe RBM-ek segítségével ami mindemellé még jellemzõ kiemelést is végzett és ott tanítottam rajta a szupport vektor gépet, remélve hogy jó osztályozási eredményeket kapok. Ezt két megközelítésben próbáltam meg, az egyik az volt amikor egyetlen RBM-em volt, 3072 bemeneti és 783 kimeneti neuronnal, a másik az volt amikor 3 RBM-et tanítottam meg, minden színcsatornára egyet, és utána ezeket egy tömbbé összefûzve adtam át az SVM-nek. A két megközelítést a \figref{svmRbm}~ábra szemlélteti.

\begin{figure}[!ht]
\centering
\includegraphics[width=100mm, keepaspectratio]{figures/svmRbm}
\caption{} 
\label{fig:svmRbm}
\end{figure}

\subsection{A Konvolúciós modell}
Több konvolúciós modellt kipróbáltam, a méréseim egyik fõ iránya az volt hogy ugyanazt a modellt probálom ki az MNIST és a CIFAR-10 adathalmazon, és megnéznem hogy melyik modell hogyan reagál az adat megnövekedett komplexitására. Több hálózatot kipróbáltam, az alapmodell felépítését a \figref{convnet_1}~ábra mutatja. Arra is irányultak kísérleteim hogy plusz teljesen összekötött rétegek hozzáadása, esetleg a konvolúciós rétegek más elrendezése milyen eredményre juttat.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/convnet_1}
\caption{} 
\label{fig:convnet_1}
\end{figure}

Az egyes operációkat szeretném megmagyarázni röviden amiket a konvolúciós modelljeimben használtam:
\begin{itemize}
\item \emph{conv(magasság, szélesség, mélység):} Kétdimenziós, diszkrét konvolúció. Ahol a magasság és a szélesség adják meg a képfolt nagyságát. A mélység pedig hogy hány neuron legyen az adott rétegben. A kétdimenziós konvolúció definícióját a \eqref{convDef}-képlet mutatja be.
\item \emph{maxpool(magasság, szélesség)} Egy alulmintavételezési operátor a térbeli dimenziókban(magasság, szélesség). Ezzel csökkentjük drasztikusan a jellemzõterek nagyságát és adunk transzlációs invarianciát a rendszerhez. Mindig a legerõsebb jelet viszi keresztül a pooling foltból.
\item \emph{fc(neuronok száma):} A teljesen kapcsolt rétegeket a hálózat végére kapcsoljuk. Amíg a konvolúciós rétegek egy magas absztrakcióval rendelkezõ, viszonylag transzláció invariáns jellemzõteret nyújtanak a képrõl, addig a hálózat végén lévõ teljesen kapcsolt rétegek teszik lehetõvé a nemlineáris leképzéseket ebben a jellemzõtérben. Az itteni teljesen kapcsolt réteget és a hozzá tartozó nemlineritásokat is a \eqref{reLu}, \eqref{sigmoid} írja le.
\item \emph{lrn:} Local Response Normalization, az alexNet\cite{alexNet} publikációból kiderül, hogy ez a lokális normalizálási eljárás statisztikailag szignifikáns javulást képest hozni a hálózat osztályozó képességében, mert csökkenti a szaturációt a relu rétegekben. Ezt a matematikai struktúrát a \eqref{lrn}~képlet szemlélteti. Ez is egy valós biológiai jelenség - a laterális inhibició - által bevezetett technika.
\item \emph{flatten:} Az elsõ teljesen kapcsolt, és az utolsó konvolúciós réteg között ki kell lapítani a 3 dimenziós konvolúciós struktúrát egy vektorba hogy össze lehessen kapcsolni a következõ réteggel. Ezt az operációt hívjuk angolul flattennek.
\end{itemize}

\begin{align}
\label{eq:convDef}
h^k_{ij} = activation((W^k * x)_{ij} + b_{k}) \quad ahol \quad * \text{ konvolúciós operátor.}
\end{align}
\begin{align} \label{eq:lrn}
b^i_{x,y} = a^i_{x,y}/\Bigg(k + \alpha\sum^{min(N-1,i+n/2)}_{j=max(0,i-n/2)}(a^j_{x,y})^2\Bigg)^\beta
\end{align}

Ahol $a^i_{x,y}$-nél az $i$-edik kernel-t alkalmazzuk az $(x,y)$ beli pozícióra, $b^i_{x,y}$ a normalizáció utáni eredmény, $n$ a vizsgált pozíció szomszédos kernel térképei, $N$ az összes kernel a rétegben, $\alpha$, $\beta$ és $k$ pedig további hiperparaméterek.

\subsection{A konvolúciós modell skálázása}
\paragraph{Indoklás} Az elõbbiekben bemutatott konvolúciós modell, és amikkel általánosságban a szakdolgozat keretén belül kísérleteztem könnyedén taníthatóak pár perc/óra alatt egy korszerû CPU-n. Viszont ha az ember nagyobb modelleket szeretne tanítani akkor ez már nem egy reális alternatíva. Ezekre az esetekre egy nagyobb hálózattal kísérleteztem a CIFAR-10 adathalmazon amit az alábbi instrukciók alapján készítettem el\cite{scalingCNN}, aminek a felépítését a \figref{convnet_2}~ábra mutatja. 

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/convnet_2}
\caption{} 
\label{fig:convnet_2}
\end{figure}

\paragraph{Használt eszközök}Ezt a hálózatot teszteltem CPU-n illetve a laptopom NVIDIA 740M típusú videokártyáján, ezen felül a Tensorflow oldalán találtam adatokat egy NVIDIA Tesla K40c-n futtatott scientific accelerator cardon elért eredményekrõl is ugyanezen az adathalmazon. A program erõsen párhuzamosított volt, és a felépítése lehetõvé tette volna tetszõleges számú GPU-ra való kiterjesztését, ennek a program felépítésének a részleteit szeretném a következõ bekezdésben megosztani.
\paragraph{A számítás elosztása} A Tensorflow képes a számításokat automatikusan elosztani az eszközök között olyan módon hogy heurisztikusan megkeresi hogy mely operációk mentén érdemes szétvágni a számítási gráfot, majd azokhoz az élekhez berak küldõ és fogadó csomópontokat ahogyan a \figref{device_distro}~ábra szemlélteti.
Ez sok esetben teljesen megfelel az elvárásainknak, ha nem szeretnénk sokat veszõdni a hálózat elosztásával, vagy amúgy is túl nagy a hálónk, és nem férne el gradiens számítással együtt rendesen egyetlen eszközön. De ha kisebb a háló és több eszközünk van akkor felmerülhet a lehetõség hogy esetleg jobban megérné a gráfot egy az egyben lemásolni az egyes eszközökre, és a súlyokat a fõ memóriában tartani, majd az egyes párhuzamos futások után itt szinkronizálni a lefutásokat. Én ezt a megközelítést teszteltem le, aminek a neve torony párhuzamos tanítás, ezt a \figref{tower_parallel}~ábra mutatja.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/device_distro}
\caption{Forrás: \cite{tensorflow}} 
\label{fig:device_distro}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/tower_parallel}
\caption{Forrás:\cite{scalingCNN}} 
\label{fig:tower_parallel}
\end{figure}

\paragraph{Adatok dinamikus felolvasása} Természetesen az a kérdés is felmerülhet az emberben hogy ez nagyon jó hogy így el tudjuk osztani a számításokat, de az is probléma lehet ha a tanuló adatok nem férnek be a memóriába, akkor nem tudjuk rendesen tanítani õket, kivéve ha valamilyen bonyolult felolvasási kódot írunk hozzá. Hál istennek ezt nem kell megírnunk, hanem be van építve a keretrendszerbe, a Tensorflow erre direkt felolvasási pipelineokat állít a programozó rendelkezésére ennek a felépítését szemlélteti a \figref{tfPipeline}~ábra. Egy ilyen pipelinenak a következõ paraméterei vannak:
\begin{itemize}
\item Bemeneti file nevek.
\item A felolvasásra és elõfeldolgozásra használni kívánt szálak száma.
\item A megállási kritérium
\item A pipeline mérete, hogy hány még meg nem tanított kép legyen mindig a pipelineban.
\end{itemize}
Ezek után a keretrendszer gondoskodik arról hogy a szálak mind stratifikáltan, külön fileokból olvassanak fel, illetve ha példányosítunk egy koordinátor objektumot és beregisztráljuk hozzá a pipeline-t akkor a szálak közötti hibakezelésrõl is gondoskodik, hogy egy szál hibája esetén ne álljon le hibával az egész tanítási folyamat.
Ezen felül definiálhatunk még elõfeldolgozási lépéseket minden képhez, az általam használt implementációban például egyszerre 20 feldolgozatlan képet tart a memóriában, ezeket elõször közelítõleg fehéríti (ez is beépített funkció), majd ezt az adathalmazt négyszerezi a következõ képpen:
\begin{itemize}
\item A képek véletlenszerû tükrözése.
\item A kép világosságába való véletlenszerû zaj bevezetése.
\item A kép kontrasztjának véletlenszerû torzítása.
\end{itemize}
Ez jelentõsen növeli a háló általánosító képességét. Majd ezeket az adatokat beadja egy új sorba, ami véletlenszerû batcheket generál a 16 szál képeibõl, és megadhatjuk hogy hány modell vegye kis és dolgozza fel õket egyszerre. Esetemben a torony-párhuzamos elrendezésben ez a GPU-k számától függ.

\begin{figure}[!ht]
\centering
\includegraphics[width=120mm, keepaspectratio]{figures/tfPipeline}
\caption{Forrás:\cite{TensorflowPipeline}} 
\label{fig:tfPipeline}
\end{figure}

